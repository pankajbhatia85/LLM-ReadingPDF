{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panka\\OneDrive\\Desktop\\LLM\\RAG\\Read_PDF\\LLM-ReadingPDF\\PDF_Read1.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs = {'device': 'cpu'})\n",
      "c:\\Users\\panka\\OneDrive\\Desktop\\LLM\\RAG\\Read_PDF\\LLM-ReadingPDF\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.vectorstores import FAISS,faiss\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings,HuggingFaceInstructEmbeddings,HuggingFaceHubEmbeddings,OpenAIEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.chains import QAWithSourcesChain,RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from PDF_Read1 import create_index1,uploaded_docs,load_model,new_model,prepare_data\n",
    "from summary_functions import prepare_data_summarize,get_page_summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPEN_API_KEY=os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=OpenAI(api_key=OPEN_API_KEY)\n",
    "sys_prompt= \"\"\"You are a research analyst and you are expert in reading the text of the document and summarize it in to 300 words.\"\"\"\n",
    "#user_prompt=\"\"\"You are provided with the PDF document your task is to summarize the provided document in 300 words\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Providing a PDF file which is summarized page-by-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pdf,len=uploaded_docs(\"./LLM_Fine_Tuned_News.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' \\nEnhancing LLM with Evolutionary Fine -Tuning for News \\nSummary Generation  \\nLe Xiao, Xiaolin Chen  \\nCollege of Information Science and Engineering, Henan University of Technology, \\nZhengzhou, China  \\nxiaole@haut.edu.cn chenxiaolin @stu.haut.edu.cn  \\nAbstract  \\nNew s summary generation is an \\nimportant task in the field of intelligence \\nanalysis, which can provide accurate and \\ncomprehensive information to help \\npeople better understand and respond to \\ncomplex real -world events. However, \\ntraditional news summary generatio n \\nmethods face some challenges, which are \\nlimited by the model itself and the \\namount of training data, as well as the \\ninfluence of text noise, making it difficult \\nto generate reliable information \\naccurately. In this paper, we propose a \\nnew paradigm for new s summary \\ngeneration using LLM  with powerful \\nnatural language understanding and \\ngenerative capabilities. We use LLM  to \\nextract multiple structured event patterns \\nfrom the events contained in news \\nparagraphs, evolve the event pattern \\npopulation with genetic  algorithm, and \\nselect the most adaptive event pattern to \\ninput into the LLM  to generate news \\nsummaries. A News Summary Generator  \\n(NSG ) is designed to select and evolve \\nthe event pattern populations and \\ngenerate news summaries. The \\nexperimental results sho w that the news \\nsummary generator is able to generate \\naccurate and reliable news summaries \\nwith some generalization ability.  \\n 1. Introduction  \\nIn recent years, pre -trained language models \\nhave undergone rapid development [1, 2, 3, 4, \\n5, 6], especially models b ased on the \\nTransformer [7] architecture, which has the \\nability to process natural language. These \\nmodels are able to automatically learn \\nstatistical patterns and patterns in language by \\ntraining on large -scale textual data , which \\nmakes pre -trained languag e models widely \\nadaptable and can be applied to a variety of \\ndomains and tasks.  \\nLarge language m odels  (LLM)  have \\nimproved the experimental results of many \\nnatural language processing tasks, exceeding \\nthe previous state -of-the-art of deep learning \\nmodels in  tasks such as information extraction \\n[8] and causal inference [9], and therefore, how \\nto enhance the performance of LLMs in \\nspecific tasks has attracted extensive research.  \\nNews summary generation is a type of \\ndocument summary generation [10], which \\naims to generate concise and important event \\ntopics in a paragraph of text to better \\ncommunicate intelligent  content. It plays a key \\nrole in areas such as information processing, \\nintelligence analysis, research , and decision -\\nmaking . Automatic news summary gener ation \\ncan provide accurate and comprehensive \\ninformation to help people better understand \\nand respond to complex real -world events.  \\nTraditional news summary generation is \\nmainly used to generate headlines , a task that \\nrequires the model to be able to under stand the ',\n",
       "  '1'),\n",
       " ('key information of the news text and be able to \\nexpress it in a concise manner.  \\nPrevious news headline generation methods \\n[11, 12, 13] currently have some challenges, \\ndue to the limitations of the model itself and the \\namount of training data, it is often difficult for \\nthe model to fully understand the semantics of \\nthe article, and if there is too much noise in the \\nrepresentation of events and the quality of the \\ndata set is low, it may affect the consistency of \\nthe generated headlines with the orig inal text, \\nand it is often difficult to accurately generate \\nreliable headlines for articles in certain \\nspecialized fields , event pattern as a kind of \\nstructured data with concise and accurate \\ncharacteristics . To address the above \\nchallenges, we use LLM to generate news \\nsummaries. News paragraphs often have long \\ntext segments and contain multiple events, each \\nwith an implied event pattern, as in Figure 1, to \\nenhance the summary generation capability of \\nLLM through event pattern evolution.  \\n \\nFigure 1:  Get eve nt patterns from news \\nparagraphs  \\n \\nWe propose a new paradigm for news \\nsummary generation - using LLM  to obtain \\nevent patterns from texts to construct a pool of \\nevent patterns , and with the help of the idea of \\ngenetic algorithm, we use the event patterns as \\nchromosomes, select crossover of \\nchromosomes to evolve the pool of event \\npatterns, and evaluate the chromosomes in the \\npool by the fitness function The highest fitness \\nmeans the best quality, and the best quality individuals are selected to enhance LLM so that \\nLLM generates accurate, reliable and \\ncomprehensive news summaries . \\nTo this end, we designed a news summary \\ngenerator (NSG) to automatically generate \\naccurate and comprehensive news summaries \\nin a three -step process:  \\n1. Unsupervised candidate event pat tern pool \\ngeneration. In the unsupervised case, multiple \\nevents are extracted from the news text and a \\npool of event patterns is constructed based on \\ncontextual fine -tuning of the LLM.  \\n2. Candidate event pattern population \\nevolution.  A genetic algorithm is  used to evolve \\nthe event pattern pool by selecting crossover \\nfor event patterns.  \\n3. Fine -tuning enhanced LLM to generate \\nnews summaries . The most adapted event \\npattern is selected to generate news summaries \\nusing LLM.  \\nMain contributions to this article:  \\nWe propose a new news summary generation \\nparadigm , i.e., with evolutionary knowledge, \\nfine-tuning LLM for enhancement to improve \\nthe news summary generation capability of \\nLLM and effectively address the challenges of \\ninaccurate , incomplete , and poorly \\ninterpretable news summaries generated by \\nLLM.  \\nDesign a news summary generator to \\ngenerate candidate event pattern pools from \\nopen -world  knowledge using LLM, iteratively \\nupdate the event pattern individuals to evolve \\nthe candidate event pattern pools, and selec t the \\nmost adaptive individuals to enhance LLM to \\ngenerate accurate and comprehensive news \\nsummaries . \\nExperiments have proven that news \\nsummary generators can generate accurate and \\ncomprehensive news summaries from news \\ntexts , providing concise and accurat e natural \\nlanguage descriptions to better understand and \\nconvey the core content of events, and are \\nvaluable for intelligence analysis.  \\n',\n",
       "  '2'),\n",
       " ('2. Background and Related Work \\nThis section reviews related research on news \\nsummary generation , event extraction , and \\nknow ledge acquisition using LLM . Genetic \\nalgorithms are also introduced.  \\n2.1 Research on Event Extraction  \\nEvent extraction is an important task in the \\nfield of natural language processing, aiming at \\nextracting key information about events from \\ntext. The event extraction task can be divided \\ninto two main areas:  \\n1. Trigger Identification: The goal of this \\ntask is to identify keywords or phrases in the \\ntext that indicate the occurrence of an event, \\nalso known as event trigger words. Trigger \\nwords are usually verbs  or noun phrases that \\ncan explicitly or implicitly indicate the \\noccurrence of an event. Identifying trigger \\nwords is the first step in event extraction and \\nhelps to identify the events present in the text \\n[14]. \\n2. Event Argument Identification and \\nClassifi cation: Once the event trigger word is \\nidentified, the next task is to identify the \\narguments associated with the event, i.e., \\ninformation about the participants, time, and \\nplace of the event. The event argument \\nelements can be noun phrases or verb phrases , \\nwhich have relationships with the event trigger \\nwords, such as subject -action, action -object, \\netc. \\nThe event extraction task\\'s challenge is \\ndealing  with semantic complexity, polysemy , \\nand context dependency. A combination of \\ntechniques such as lexical an notation, \\ndependent syntactic analysis, entity recognition , \\nand semantic role annotation is required to \\nidentify event trigger words and event thesis \\nelements and to classify and associate them. \\nEvent extraction is of great value in \\napplications such as in formation extraction, \\ntext understanding , and knowledge graph \\nconstruction, and helps to extract and organize \\nevent information from large -scale text data to support various practical application scenarios.  \\nTraditional machine learning -based  event \\nextracti on tasks can be divided into pipeline -\\nbased event extraction and union -based event \\nextraction according to the solution process of \\nthese above tasks [15]. The pipeline -based \\napproach treats all subtasks as independent \\nclassification problems:  [16, 17] , and the joint -\\nbased approach [ 18, 19 ]. \\nThe set of event types and their meta -roles \\nobtained from event extraction constitutes an \\nevent model that helps to completely describe \\nthe event and understand its aspects , in the form \\nof \"type: bombing ; meta -roles : per petrator, \\nvictim, target, tool\" [20] . \\nThe event schema  consists of two parts: \\nevent type describes the general category or set \\nof categories to which the event belongs , and \\nmeta -roles refer to the semantic roles or \\nfunctions associated with the event in di fferent \\nparts of the event schema. Each theoretical \\nmeta -role represents an element or role in the \\nevent, such as the performer of the action, the \\nbearer of the action, the object of influence of \\nthe action, etc. They describe the relationships \\nand roles b etween different participants in the \\nevent. Argumentative meta -roles are usually \\nabstract and aim to represent universal \\nconcepts or role types in an event . Common \\nmeta -roles include Subject, Object, Time , Place , \\nActions , etc. By defining and identifying \\ndifferent theoretical meta -roles, the \\ncomponents of an event can be more accurately \\ndescribed and the semantic analysis and \\nunderstanding of the event can be performed.  \\nEvent patterns provide a structured way to \\nrepresent and describe events. Through event \\npatterns, we can capture important information \\nabout events, understand and analyze the core \\ncontent, relationships , and characteristics of \\nevents , and identify the key elements and \\nplayers of events.  \\nNatural language generation is based on \\nevent patterns,  i.e., generating corresponding ',\n",
       "  '3'),\n",
       " (\"natural language descriptions or sentences \\nbased on event patterns to generate specific and \\naccurate representations of events for \\ngenerating news headlines, summaries, reports, \\netc. \\n2.2 Genetic Algorithm  \\nGenetic algorithm [22] is an optimization \\nalgorithm based on the theory of biological \\nevolution, which simulates the evolutionary \\nprocess in nature and searches and optimizes \\nthe solution space of a problem through genetic \\noperations (e.g., selection, crossover, and \\nmutation ). \\nThe basic idea is to create a set of individuals \\n(called a population), each of which represents \\na candidate solution to the problem. By \\niterating generation by generation, the \\nsolutions in the population are improved by \\nselecting good individuals, cros sover and \\nvariation operations, and gradually \\napproximating the optimal solution.  \\nIn this paper, we use LLM to generate \\noriginal populations and evolve individuals by \\nselection and crossover in the following \\nprocess : \\n1. Initial population generation: LLM \\ngenerates event patterns as individuals from \\nevent representations based on contextual fine-\\ntuning  unsupervised , and constructs a pool of \\ncandidate event patterns as the original \\npopulation . \\n2. Adaptation assessment: Each individual is \\nassessed using an ada ptation function that \\nmeasures accuracy and comprehensiveness \\n(metrics) . \\n3. Selection: Based on the evaluation results \\nof the fitness function, the individual with \\nhigher fitness is selected as the parent solution \\nand used to generate the next-generation  \\nsolution.  \\n4. Crossover : The gene crossover operation \\nis performed on the newly generated solutions \\nto increase the exploitability  of the solution \\nspace by introducing randomness.  5. Update the population: replace or merge \\nthe newly generated individuals with the parent \\npopulation to form an updated pool of event \\npatterns . \\n6. Repeat steps 2 to 5: iteratively perform \\nselection and crossover operations until the \\ntermination conditions are met .  \\nAfter the evolutionary process of the genetic \\nalgorithm, the event patterns with the highest \\nfitness are filtered out in the pool of candidate \\nevent patterns. These event patterns have high \\nfitness values and quality to summarize the \\ntext's important events and key information . \\n2.3 Text Summary Generation  \\nNatural language  expressions of events are \\ndiverse and sparse, and summarizing news \\nsummaries , i.e., unifying different event \\nrepresentations and presenting the core content \\nof events with concise and accurate natural \\nlanguage descriptions, is of great value for \\nintellige nce analysis. Summarizing text can \\nhelp one quickly understand and organize large \\namounts of intelligence data, extract key \\ninformation, avoid drowning in noise, and thus \\nunderstand and convey the main points of an \\nevent more effectively. This un ified and \\nsummarized description helps eliminate \\ninformation redundancy and ambiguity, \\nimproves the efficiency and accuracy of \\nintelligence analysis, and provides strong \\nsupport for decision -making.  \\nBy summarizing news summaries, a large \\namount of heteroge neous information can be \\nbetter processed and utilized to provide \\npowerful guidance and decision support for \\nintelligence work. Often, intelligence analysis \\nwithin niche domains has similar needs, \\nrequiring the abstraction of a unified summary  \\nfrom a large  number of similar events.  \\nTraditional news summaries are generated \\nusing rules and templates designed by  human \\nexperts  to generate news summaries and \\nprobability -based methods, including the use of \\ntext clustering to group related events into pre -\",\n",
       "  '4'),\n",
       " ('defined summaries based on manually edited \\nsummaries, in addition to methods that use the \\nlongest common subsequence to generate a \\nrepresentation of news summaries [11] but are \\nmore affected by data sparsity . \\nWith the development of deep learning \\ntechniques, neura l network -based approaches \\n[23] have gradually become the mainstream of \\nnews summary generation, which can better \\nhandle the semantic and contextual complexity \\nand overcome the problem of data sparsity, but \\noften lead to overfitting due to the \\nunavailabili ty of large -scale training data [24]. \\nWith the rise of LLM , they have shown good \\nresults on many natural language processing \\ntasks, but the black -box nature of LLM  causes \\na lack of interpretability in the results they \\ngenerate, which makes it difficult to gain \\ninsight into how the models generate news \\nsummaries from the input text data.  \\nThe interpretation of event patterns can also \\nprovide a basis for evaluating and interpreting \\nthe generated news summaries. Using event \\npatterns to explain the generation of  news \\nsummaries can help us understand how LLM  \\ngenerates  high-quality news summaries based \\non event -related information , and we can gain \\na clearer understanding of how the models \\nunderstand and represent key features, \\nsemantic relationships, and contextual  \\ninformation about events.  \\n2.4 Knowledge Acquisition with LLM  \\nThe advantage of using LLM  for knowledge \\nacquisition lies in their ability to understand \\nand generate language and to learn and extract \\nknowledge from large -scale text data . \\nCao [ 21] et al. prop osed five cycles of \\nknowledge in LLM: acquisition, representation, \\nexploration, editing, and application.  \\nIn the field of news summary generation, \\nLLM  can generate accurate and concise text \\nsummaries by understanding and semantic \\nanalysis of text. It can h elp extract the key \\ninformation and core content in the text, thus enabling knowledge distillation and acquisition. \\nHowever, it should be noted that LLM  also has \\nsome limitations, such as the quality of the \\ngenerated news summary representations , and \\nchall enges such as accuracy and coverage.  \\nTo improve the quality of LLM -generated \\nnews summary representations and to address \\nthe challenges of accuracy and \\ncomprehensiveness, this paper proposes an \\nLLM -based news summary generation \\nparadigm and designs a news summary \\ngenerator based on it to generate high -quality, \\naccurate, and comprehensive news summary \\nrepresentations from a large number of event \\nrepresentations.  \\n3. News Summary  Generation  \\nThis section describes the method of news \\nsummary generation using the large language \\nmodel.  \\nSince a piece of news text often has a large \\namount of data, direct generation of event \\npatterns may lead to the introduction of noise \\nand phantom data due to too many \\nargumentative roles and may cause important \\nargumentative elements t o be missing, so we \\nextract different patterns from the different \\nevent contents contained in it and use a genetic \\nalgorithm to evolve the extracted event patterns \\nand evaluate the final patterns by the fitness \\nfunction, and the pattern with the highest sc ore \\nThe highest scoring patterns are input to LLM \\nto generate news summaries . \\nFormally, given the corpus and LLM, there  \\nare N news  fragments in the corpus { T 1, T2, ..., \\nTN }, each news fragment can extract multiple \\nevents , LLM obtains an event pattern from \\neach event in the news fragment Tn, n ∈ N \\nand constructs a pool of event patterns 𝑝𝑛 as \\nthe original population for evolution , 𝑝𝑛 \\n={𝑠𝑛1 , 𝑠𝑛2 , ... ,𝑠𝑛𝑖 ..𝑠𝑛𝑖  represents the i -th event \\npattern in the n -th event pattern pool . \\nIn this paper, we use a genetic algo rithm to \\nevolve the population and select the crossover ',\n",
       "  '5'),\n",
       " (\"operations of the theoretical roles contained in \\nthe individuals . Based on the above method , we \\ndesign the News Summary Generator (NSG ) to enhance the LLM news summary generation, \\nand the framework is  shown in Figure 2. \\nFigure 2 The Framework of NSG  \\n \\nThe News Digest Generator contains three \\ncomponents:  \\n1. Candidate event pattern pool generation.  \\n2. Event pattern population evolution.  \\n3. News summary generation . \\nIn the following , we will describe t hese \\nsections in detail.  \\n3.1 Candidate Event Pattern Pool \\nGeneration  \\nEvent patterns are the basis for generating news \\nsummaries, providing abstraction and \\ngeneralization of event representations.  By \\nacquiring event patterns, key information, \\nthemes , and co re contents can be extracted \\nfrom a large number of event representations, \\nthus enabling efficient understanding and \\nsummarization of events.  \\nReal-world text data usually contains noise \\nand diversity, including misspellings, irregular \\ngrammar, randomness, etc. These factors \\nincrease the difficulty of accurately extracting event patterns from text, requiring models with \\ncertain robustness and generalization \\ncapabilities. Event patterns often exist in the \\nimplicit information of text rather than explicit \\nrepresentations , and the model needs to be able \\nto understand contextual dependencies and \\ninfer the implicit event information in order to \\ncapture event patterns accurately. Traditional \\nneural network -based event pattern extraction \\nmethods to acquire event pat terns require a \\nlarge amount of training data, and in some \\ndomains or specific event types, relevant data \\nmay be very limited , and this data scarcity can \\nlimit the model's ability to learn and understand \\nevent patterns.   \\nLLM is equipped with powerful langu age \\ngeneration and context learning capabilities \\nthrough the training of large -scale text data. \\nBased on this, this paper uses LLM to \\nunsupervised  and automatically generate \\ncandidate event patterns from the original event \\n\",\n",
       "  '6'),\n",
       " ('representations.  \\nBased on this fr amework, we use LLM for \\ncontextual fine-tuning . First let LLM select an \\noverall event type for the news to generalize, \\nthen identify different events from the text and \\nget as many event patterns as we need based on \\neach event. The purpose of context fine -tuning \\nis to improve the performance and \\ngeneralization of the model on a specific task \\nand to generate results as we need them . The \\nadvantage of context fine -tuning is that it can \\ntake advantage of the general language \\nknowledge already learned by the pre -trained \\nlanguage model and fine -tune it on a specific \\ntask, and by fine -tuning the model on task -\\nrelevant data, it allows the model to better \\nunderstand the task -specific semantics and \\ncontext and to generate task -relevant output.  \\nEach context is a text pa ttern pair containing \\nan event representation and the event patterns \\nobtained from that event representation, which \\nis used to guide the LLM  for a generation. LLM \\nunsupervised  acquires event patterns from \\ndifferent types of events based on contextual \\nfine-tuning, and constructs a pool of N event \\npatterns based on N news segments in the \\ncorpus, with each event pattern pool evolving \\nas a primitive population . \\n3.2 Knowledge Evolution  \\nIn this paper, we use the idea of genetic \\nalgorithm to generate news summarie s by \\ncombining good genomes between individuals \\nof different parents through genetic \\nrecombination to produce offspring with higher \\nadaptability , select crossover by using the \\ntheoretical meta -role in event patterns as genes, \\nand finally select the event p attern with the \\nhighest adaptability from the population . The \\npool of event patterns generated in the previous \\nsection provides the raw material for the \\nevolution of event pattern selection.  \\nIn this paper , a pool of N event patterns \\nobtained from N news cl ips is used as the \\noriginal population , and the event patterns in the event pattern pool 𝑝𝑛  corresponding to \\nnews clips Tn, n ∈ N are used as \\nchromosomes , which are formally the set of \\nevent types and argument roles : {Type; \\nArguments. . In each generation, all \\nchromosomes in the population need to be \\nevaluated using the fitness function. \\nChromosomes with higher fitness values are \\nplaced in the mating pool, replacing \\nchromosomes with lower fitness to update the \\npopulation . \\nIn this paper, we evaluate the salience and \\nreliability of the thesis  meta -role in terms of \\nfrequency of occurrence and i mportance , \\nrespectively, and design the fitness function. \\nThe salience and reliability of the thesis meta -\\nrole determine the accuracy and \\ncomprehensiveness of the generated summary.  \\nThe TF -IDF score is the event pattern 𝑠𝑛𝑖 \\nratio of the frequency of occurrence in the \\ncurrent population to the frequency of \\noccurrence in the full population. The formula \\nis as follows:  \\nF(𝑠𝑛𝑖)=(1+log (freq (𝑠𝑛𝑖))2)∗\\nlog(𝑁\\n∑𝑓𝑟𝑒𝑞 (𝑠𝑛𝑖)𝑁𝑛)                     (1) \\nfreq (𝑠𝑛𝑖) for the thesis meta -role 𝑠𝑛𝑖 \\nfrequency of occurrence in P n. \\nThe TextRank [25] score treats a document \\nas a network of words and the links in the \\nnetwork as semantic relationships between the \\ntheoretical roles, with the follo wing equation:  \\nW(𝑠𝑛𝑖)=(1−𝑑)+𝑑∗∑𝑠𝑛𝑗∈\\n𝐼𝑛(𝑠𝑛𝑖)𝑤𝑖𝑗\\n∑𝑠𝑛𝑘∈𝑂𝑢𝑡(𝑠𝑛𝑗)𝑤𝑗𝑘∗𝑊(𝑠𝑛𝑗)        (2) \\nThe fitness function Q is defined as the \\nweighting of the TFIDF score and TextRank \\nscore of the argume ntative meta -role in the \\nevent pattern.  The formula is as follows:  \\n             Q=α∗F(𝑠𝑛𝑖)+β∗W(𝑠𝑛𝑖)       (3) \\nα and β is are the hyperparameters , and d \\nis the damping factor.  \\nChromosomes with the highest scores are \\nconsidered to be the best c urrent solution. The ',\n",
       "  '7'),\n",
       " ('selection of chromosomes with higher fitness \\nallows the genetic algorithm to select the \\nhighest -quality  event patterns.  \\nFor the selection of parents, the fitness of \\neach event pattern in the population was first \\nevaluated according to Equation ( 3), and a \\nportion of individuals were selected as parents \\nto participate in the next evolution according to \\nthe fitness value using the roulette algorithm \\n[26], where event patterns with higher fitness \\nhave a higher probability of being selected,  \\nwhile event patterns with lower fitness still \\nhave a certain probability of being selected, \\nwhich is conducive to maintaining the diversity \\nof the population . \\nSince the event patterns contain fewer and \\nless diverse argument roles, introducing \\nvariation op erations at this time may introduce \\nunnecessary randomness , so using only \\ncrossover operations here can maintain the \\nstability of the population and allow faster \\nconvergence to the optimal solution. The \\ncrossover operation combines the \\nchromosomes in the p ool randomly two by two \\ninto event pattern pairs and swaps its own k \\nargument roles , k being a random number no \\ngreater than the minimum number of argument \\nroles contained in the two event patterns . \\nSubsequently, the progeny individuals \\nobtained by crossov er are merged with the \\nparent individuals to form a new generation of \\npopulation. The new generation population is \\ninvolved in the selection and crossover \\noperation as the parent of the next iteration. In \\nthis paper, we set the maximum number of \\niterations  I, I am the hyperparameter , and the \\nalgorithm stops running when this number is \\nreached, and finally , the event patterns in the \\npopulation are ranked according to the fitness \\nvalue, and the individual with the highest \\nfitness value is selected to represen t the most \\naccurate and comprehensive event pattern with \\nthe highest quality as a hint for news summary \\ngeneration.  3.3 News Summary Generation  \\nWe use 3.2 to obtain highly adaptable event \\npatterns and input the highly adaptable event \\npatterns into the LLM  to generate more \\naccurate and comprehensive news summaries . \\nThe black -box characteristics  of the LLM  also \\ncause  the generated results to often lack \\ninterpretability. The event model can provide \\nexplanations for the news summaries  generated \\nby the LLM , thus  increasing their \\ninterpretability and making the generated \\nresults easier to understand and interpret.  \\nIn the second step, we extract a large number \\nof event patterns from the event representations \\ncontained in each news clip and evolve the \\npopulation con tinuously, and after a certain \\nnumber of evolutions sort the event pattern \\nwith the highest fitness s as the high-quality  \\nevent pattern. n news clips are obtained with a \\ntotal of N high-quality  event patterns  S = \\n{s1 ,s2 ,...,sN }. The set of high -quality event \\npatterns is then input into the large language \\nmodel to obtain news summaries . \\nThe quality of the generated news \\nsummaries is assessed by a series of metrics \\nsuch as ROUGE, BLEU , and Overlap scores.  \\nBy combining high -quality event patterns \\nwith the generative power of large language \\nmodels, we are able to obtain more accurate \\nand comprehensible news summaries , resulting \\nin better results in terms of information \\nconveyance and understanding. This approach \\nnot only improves the quality of news \\nsummaries but also enhances our \\nunderstanding of the news summary generation \\nprocess.  \\n4. Experiment  \\n4.1 Experimental Preparation  \\nDataset : \\nWe experimented with the PENS [27] news \\nheadline generation dataset , which contains \\n113,762 news articles in 15 topic typ es, each \\ncontaining id, headline, content and a category ',\n",
       "  '8'),\n",
       " ('tag, of which we only used the news headline \\nand content . The grain storage pest event \\ncorpus constructed by Xiao Le [ 28] et al. is also \\nused as a niche domain dataset for comparison \\nto explore the reliability of the summaries \\ngenerated by the news summary generator \\nproposed in this paper on a niche specialized \\ndomain, which is a grain storage pest event \\ncorpus including event types and event texts \\nobtained from grain storage pest texts after \\nevent ex traction.  \\nLLM : \\nWe use ChatGLM [29] as the backbone large \\nlanguage model of our news summarization \\nframework.  \\nThe effectiveness of the method proposed in \\nthis paper was verified by experimenting with \\ntwo different tasks:  \\n(1) The event pattern getter acquire s and \\nevolves the event pattern quality enhancement \\neffect afterward . \\n(2) The experimental results of the \\nframework on the niche domain dataset are \\ncompared with the PENS news headline \\ngeneration  dataset to demonstrate the \\ngeneralization ability of the obt ained news \\nsummaries.  \\n4.2 Results of News summary generator  \\nIn this section, the quality of news summaries \\ngenerated by NSG is evaluated, and the \\nbaseline selection of the TFIDF -based algorithm and TextRank -based algorithm for \\nnews summary generation. As w ell as a Gpt2 -\\nbased news summary generation method [ 30], \\nthe main idea is to generate multiple summaries \\nand select the most appropriate news summary \\nfrom them, while we use LLM to generate \\nsummaries directly on the text.  \\nExperimental evaluation metrics ar e used \\nROUGE, BLEU and overlap score, ROUGE is \\na text summarization evaluation metric based \\non n -gram and recall, which measures the \\nquality of summarization by examining aspects \\nsuch as word overlap, sentence -level similarity , \\nand sequence -level similarit y in summarization , \\nROUGE [31] metric system includes several \\nevaluation metrics, such as  ROUGE -1, \\nROUGE -2, and ROUGE -L, etc., which denote \\nmeasuring one -word matching, measuring \\nbinary word matching, and recording the \\nlongest common subsequence, respectiv ely, \\nand are widely used in the evaluation and \\ncomparison of text summarization tasks. BLEU \\n[32] is a precision -based similarity measure, \\nand the common metrics are BLEU -1, BLEU -\\n2, BLEU -3, and BLEU -4, Overlap  detects the \\nfrequency of occurrence of repeated  phrases \\nbetween the generated news headlines and the \\nreference headlines , and can measure to what \\nextent the model is copying phrases directly \\nfrom the text as summaries . \\n  ROUGE    BLEU     \\nModel  R-1 R-2 R-L B-1 B-2 B-3 B-4 Overlap  \\nTFIDF  0.265 0.087 0.183 0.241 0.132 0.129 0.048 59 \\nTextRank  0.354 0.143 0.282 0.312 0.188 0.174 0.082 52 \\nGpt2  0.398 0.182 0.357 0.39 0.261 0.208 0.117 42 \\nGLM  0.489 0.197  0.382  0.414  0.298  0.236  0.142  45 \\nGLM+NSG  0.568  0.224  0.403  0.433  0.315  0.26 0.174  43 \\nTable 1: Compari son of experimental results of GLM+NSG with baseline methods. We chose \\nROUGE , BLEU and Overlap  as evaluation metrics, and the values of Overlap are percentage s \\n \\nWe compare the news summary generation \\nmethod of LLM plus NSG proposed in this paper with the above baseline most, the effect \\nof LLM plus NSG for news summary ',\n",
       "  '9'),\n",
       " ('generation is 0.431 and 0.417 higher than \\nTFIDF and TextRank, respectively, which \\nindicates that the method of TFIDF and \\nPageRank weighting the adaptation of \\nargumentative roles and then gene rating news \\nsummaries is better than directly using TFIDF \\nand PageRank is better.  \\nAlso compared with Gpt2 and direct \\nsummary generation with a large model, the \\neffects were 0.164 and 0.079 higher, \\nrespectively, demonstrating the effectiveness of \\nextracting  event patterns before population \\nevolution.  4.3 Results in the Professional Field \\nThe experimental effect on the grain storage \\ndomain dataset , the grain storage pest dataset is \\nconstructed for the purpose of constructing a \\ngrain storage pest matter mappin g, and the \\nevent text paragraphs are short, and the data are \\nin the form of grain storage pest related events \\nand the corresponding matter , which is the \\nabstract generalization of the corresponding \\nevent, which we use as the event title for the \\nexperiment . \\n  ROUGE    BLEU     \\nModel  R-1 R-2 R-L B-1 B-2 B-3 B-4 Overlap  \\nTFIDF  0.195 0.094 0.122 0.062  0.105 0.116 0.052 63 \\nTextRank  0.221 0.113 0.168 0.274 0.131 0.134 0.063 54 \\nGpt2  0.256  0.127  0.219 0.241 0.163 0.152 0.072 38 \\nGLM  0.314 0.133 0.273 0.284 0.179 0.163 0.088 40 \\nGLM+ NSG  0.378 0.172 0.305 0.312  0.202  0.193 0.112  42 \\nTable 2: GLM+ NSG  results on stored grain pest dataset compared to baseline experiments  \\n \\nThe experimental results on the corpus of \\nstored grain pests we can see that the effect of \\nthe L LM plus NSG  approach decreases on the \\nPENS dataset, but the overall effect is still \\nbetter than the baseline, which proves the \\ngeneralization of the news summary extraction \\nparadigm proposed in this paper . On the \\nOverlap  metric, since the methods based on \\nTFIDF and TextRank are extractive methods, \\nthey are more dependent on the original data \\nwhen the amount of data is small, so the \\nobtained event summaries have a higher \\nOverlap  rate with the original text, while the \\nmethods based on GPT2 and LLM and the \\nproposed method in this paper are generative, \\nso the Overlap  rate is reduced.  \\nExperimental results show that the proposed \\nmethod in this paper still achieves good results \\non the grain storage pest dataset, proving to be \\nequally effective for summary generatio n tasks \\nin niche domains . 4. Conclusion  \\nIn this paper, we propose a new paradigm for \\nnews summary generation and design a News \\nSummary  Generator  (NSG) . Experimental \\nverif y that the proposed approach in this paper \\nis able to generate accurate and reliable news  \\nsummaries on news summary generation tasks \\nwith certain generalizations  and application to \\nrelated tasks. At the same time, by introducing \\nthe event model into the news summary \\ngeneration process of the LLM , the problem of \\nits lack of interpretability can  be solved to a \\ncertain extent, enabling us to understand and \\nexplain the internal operation of the model  \\nbetter , as well as the basis and logic of \\ngenerating news summaries. This helps to \\nimprove the credibility and reliability of the \\nLLM  and provide more  reliable and \\ninterpretable results for the news summary \\ngeneration task. ',\n",
       "  '10'),\n",
       " (' \\n \\n \\n \\nReferen ce: \\n[1] Devlin J, Chang M W, Lee K,  et al.BERT: \\nPre-training of Deep Bidirectional \\nTransformers for Language Understanding[J].  \\n2018 , pages  4171 –4186  \\n[2] Lewis  M, Liu  Y, Goyal N, et al.  BART: \\nDenoising sequence -to-sequence pretraining \\nfor natural language generation, translation,  \\nand comprehension [C]// In Proc. of ACL, \\npages 7871 –7880  \\n[3] Brown T B, Mann B, Ryder N,  et al.  \\nLanguage Models are Few -Shot Learners[J].  \\n2020 , NeurIPS  2020,  pages 6 -12 \\n[4] OpenAI. ChatGPT: Optimizing language \\nmodels for dialogue, 2022.  \\n[5] OpenAI. GPT -4 technical report. arXiv \\npreprint arXiv:2303.08774, 2023.  \\n[6] Scao T, Fan A, Akiki C, et al. BLOOM: A \\n176B -Parameter Open -Access Multilingual \\nLangua ge Model [J]. 2023 , ArXiv preprint, \\nabs/2211.05100.  \\n[7] Vaswani A, Ahazeer N, Parmar N, et al. \\nAttention is All You Need [C]// In Advances in \\nNeural Information Processing Systems 30: \\nAnnual Conference on Neural  Information \\nProcessing Systems 2017 , pages  5998-6008  \\n[8] Wei X, Cui X, Cheng N, et al. Zero -Shot \\nInformation Extraction via Chatting with \\nChatGPT  [J]. arXiv:2302.10205 , 2023  \\n[9] Kıcıman  E, Ness R, Sharma A, et al. Causal \\nReasoning and Large Language Mod els: \\nOpening a New Frontier for Causality [J]. \\narXiv:2305.00050 ,2023  \\n[10] Wang M, Tanaka H, Zhong Y .1 \\nGenerating Summaries of Multiple Technical \\nArticles[J].  2023  \\n[11] Wang W, Shi C, Yu X, et al. An extracti ve \\ntopic brief representation generation method to \\nevent [J]. Journal of Shandong University  \\n(Natural Science) , 2021,56(05):66 -75+84.  [12] Han Y, Xu X, Li B , et al.  Web News Multi -\\ndocument Summarization Based on Event \\nExtraction [J]. Journal of Chinese Infor mation \\nProcessing, 2012,26(01):58 -66. \\n[13] Atapattu T, Falkner K. A Framework for \\nTopic Generation and Labeling from MOOC \\nDiscussions [J]. Proceedings of the Third ACM \\nConference on Learning , 2016, pages 201 -204  \\n[14] Hong Y, Zhang J, Ma B,  et al.Using Cross -\\nEntity Inference to Improve Event \\nExtraction[C]//  Meeting of the Association for \\nComputational Linguistics:  Human Language \\nTechnologies.  Association for Computational \\nLinguistics, 2011.  \\n[15] LI Q,  LI J,  SHENG J,  et al. A Compact \\nSurvey on Event Extraction: Approaches and \\nApplications[J]. arXiv:2107.02126 , 2021  \\n[16] CHEN Y,  XU L,  LIU K,  et al. Event \\nExtraction via Dynamic Multi -Pooling \\nConvolutional Neural Networks [C]// \\nInternational Joint Conference on Natural \\nLanguage Processing,  2015 , pages 167-176 \\n[17] SUBBURATHINAM A,LU D, JI H,  et al. \\nCross -lingual structure transfer for relation and \\nevent[C]// International Joint Conference on \\nNatural Language Processing , 2019 , pages \\n313-325 \\n[18] ZHANG J,  QIN Y ,  ZHANG Y ,  et al.  \\nExtracting entities and events as a single task \\nusing a transition -based neural model[C]//  \\nInternational Joint Conference on Artif icial \\nIntelligence,2019 , pages 5422 -5428  \\n[19] LI D,HUANG L,JI H,  et al. Biomedical \\nevent extraction based on knowledge -driven \\ntree-lstm[C]//  Human  Language  Technologies , \\n2019 , pages 1421 -1430  \\n[20] Chambers N, Jurafsky D. Template -based \\ninformation extraction without  the templates. \\n[C]//  In Proceedings of the 49th Annual  \\nMeeting of the Association for Computational  \\nLinguistics: Human Language Technologies, \\n2011, pages  976–986 ',\n",
       "  '11'),\n",
       " ('[21] Cao B, Lin H, Han X, e t al. The Life Cycle \\nof Knowledge in Big Language Models: A \\nSurvey [J]. arXiv:2307.01189 , 2023  \\n[22] Goldberg D. E., Holland J. H. Genetic \\nalgorithms and machine learning. machine \\nlearning [J], Machine Learning , 1988, 3(2), \\npages 95-99 \\n[23] Gehring J, Auli M, Grangier D,  et al. \\nConvolutional sequence to sequence \\nlearning [J]. Proceedings of the 34th \\nInternational Conference on Machine Learning, \\n2017, pages 1243 -1252  \\n[24] Li J, Tang T, Zhao W, et al. Pretrained \\nLanguage Models for Text Generation: A \\nSurvey [J], arXiv:2201.05273 , 2022  \\n[25] R. Mihalcea and P. Tarau, TextRank: \\nBringing order into text, in Proc. 2004 \\nConference  on Empirical Methods in Natural \\nLanguage Processing, 2004,  pages  404–411 \\n[26] Lipowski A, Lipowska D.  Roulette -wheel \\nselection via stochastic acceptance[J]. Physica \\nA Statistical Mechanics & Its Applications, \\n2012, 391(6):2193 -2196.   \\n[27] Ao X, Wang X, Luo L, et al. PENS: A \\nDataset and Generic Framework for \\nPersonalized News Headline Generation [C]// \\nThe Annual Meeting of  the Association for \\nComputational Linguistics (ACL) , 2021  \\n[28] Xiao L , Chen X, Shan X.  Construction of \\nStored Grain Pest Event Evolutionary Graph  \\n[J]. Journal of the Chinese Cereals and Oils \\nAssociation , 2022  \\n[29] Du Z, Qian Y, Liu X, et al. GLM: General  \\nLanguage Model Pretraining with \\nAutoregressive Blank Infilling [J]. \\narXiv:2103.10360 , 2023  \\n[30] Mishra P, Diwan C, Srinivasa s , et al. \\nAutomatic Title Generation for Text with Pre -\\ntrained Transformer Languag e Model [C]// \\n2021 IEEE 15th International Conference on \\nSemantic Computing (ICSC), 2021, pages 17 -\\n24 \\n[31] Lin C. ROUGE: A Package for Automatic \\nEvaluation of Summa ries[C]// Post-Conference Workshop of ACL 2004  \\n[32] Papineni K , Roukos S , Ward T , et al. \\nBLEU: a Method for Automatic Evaluation of \\nMachine Translation [C]//  Proceedings of the \\n40th Annual Meeting on Association for \\nComputational Linguistics , 2002 , pages 311 -\\n318 \\n ',\n",
       "  '12')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 11876.27it/s]\n"
     ]
    }
   ],
   "source": [
    "docs=prepare_data_summarize(my_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '1'}, page_content=' \\nEnhancing LLM with Evolutionary Fine -Tuning for News \\nSummary Generation  \\nLe Xiao, Xiaolin Chen  \\nCollege of Information Science and Engineering, Henan University of Technology, \\nZhengzhou, China  \\nxiaole@haut.edu.cn chenxiaolin @stu.haut.edu.cn  \\nAbstract  \\nNew s summary generation is an \\nimportant task in the field of intelligence \\nanalysis, which can provide accurate and \\ncomprehensive information to help \\npeople better understand and respond to \\ncomplex real -world events. However, \\ntraditional news summary generatio n \\nmethods face some challenges, which are \\nlimited by the model itself and the \\namount of training data, as well as the \\ninfluence of text noise, making it difficult \\nto generate reliable information \\naccurately. In this paper, we propose a \\nnew paradigm for new s summary \\ngeneration using LLM  with powerful \\nnatural language understanding and \\ngenerative capabilities. We use LLM  to \\nextract multiple structured event patterns \\nfrom the events contained in news \\nparagraphs, evolve the event pattern \\npopulation with genetic  algorithm, and \\nselect the most adaptive event pattern to \\ninput into the LLM  to generate news \\nsummaries. A News Summary Generator  \\n(NSG ) is designed to select and evolve \\nthe event pattern populations and \\ngenerate news summaries. The \\nexperimental results sho w that the news \\nsummary generator is able to generate \\naccurate and reliable news summaries \\nwith some generalization ability.  \\n 1. Introduction  \\nIn recent years, pre -trained language models \\nhave undergone rapid development [1, 2, 3, 4, \\n5, 6], especially models b ased on the \\nTransformer [7] architecture, which has the \\nability to process natural language. These \\nmodels are able to automatically learn \\nstatistical patterns and patterns in language by \\ntraining on large -scale textual data , which \\nmakes pre -trained languag e models widely \\nadaptable and can be applied to a variety of \\ndomains and tasks.  \\nLarge language m odels  (LLM)  have \\nimproved the experimental results of many \\nnatural language processing tasks, exceeding \\nthe previous state -of-the-art of deep learning \\nmodels in  tasks such as information extraction \\n[8] and causal inference [9], and therefore, how \\nto enhance the performance of LLMs in \\nspecific tasks has attracted extensive research.  \\nNews summary generation is a type of \\ndocument summary generation [10], which \\naims to generate concise and important event \\ntopics in a paragraph of text to better \\ncommunicate intelligent  content. It plays a key \\nrole in areas such as information processing, \\nintelligence analysis, research , and decision -\\nmaking . Automatic news summary gener ation \\ncan provide accurate and comprehensive \\ninformation to help people better understand \\nand respond to complex real -world events.  \\nTraditional news summary generation is \\nmainly used to generate headlines , a task that \\nrequires the model to be able to under stand the \\n'),\n",
       " Document(metadata={'source': '2'}, page_content='key information of the news text and be able to \\nexpress it in a concise manner.  \\nPrevious news headline generation methods \\n[11, 12, 13] currently have some challenges, \\ndue to the limitations of the model itself and the \\namount of training data, it is often difficult for \\nthe model to fully understand the semantics of \\nthe article, and if there is too much noise in the \\nrepresentation of events and the quality of the \\ndata set is low, it may affect the consistency of \\nthe generated headlines with the orig inal text, \\nand it is often difficult to accurately generate \\nreliable headlines for articles in certain \\nspecialized fields , event pattern as a kind of \\nstructured data with concise and accurate \\ncharacteristics . To address the above \\nchallenges, we use LLM to generate news \\nsummaries. News paragraphs often have long \\ntext segments and contain multiple events, each \\nwith an implied event pattern, as in Figure 1, to \\nenhance the summary generation capability of \\nLLM through event pattern evolution.  \\n \\nFigure 1:  Get eve nt patterns from news \\nparagraphs  \\n \\nWe propose a new paradigm for news \\nsummary generation - using LLM  to obtain \\nevent patterns from texts to construct a pool of \\nevent patterns , and with the help of the idea of \\ngenetic algorithm, we use the event patterns as \\nchromosomes, select crossover of \\nchromosomes to evolve the pool of event \\npatterns, and evaluate the chromosomes in the \\npool by the fitness function The highest fitness \\nmeans the best quality, and the best quality individuals are selected to enhance LLM so that \\nLLM generates accurate, reliable and \\ncomprehensive news summaries . \\nTo this end, we designed a news summary \\ngenerator (NSG) to automatically generate \\naccurate and comprehensive news summaries \\nin a three -step process:  \\n1. Unsupervised candidate event pat tern pool \\ngeneration. In the unsupervised case, multiple \\nevents are extracted from the news text and a \\npool of event patterns is constructed based on \\ncontextual fine -tuning of the LLM.  \\n2. Candidate event pattern population \\nevolution.  A genetic algorithm is  used to evolve \\nthe event pattern pool by selecting crossover \\nfor event patterns.  \\n3. Fine -tuning enhanced LLM to generate \\nnews summaries . The most adapted event \\npattern is selected to generate news summaries \\nusing LLM.  \\nMain contributions to this article:  \\nWe propose a new news summary generation \\nparadigm , i.e., with evolutionary knowledge, \\nfine-tuning LLM for enhancement to improve \\nthe news summary generation capability of \\nLLM and effectively address the challenges of \\ninaccurate , incomplete , and poorly \\ninterpretable news summaries generated by \\nLLM.  \\nDesign a news summary generator to \\ngenerate candidate event pattern pools from \\nopen -world  knowledge using LLM, iteratively \\nupdate the event pattern individuals to evolve \\nthe candidate event pattern pools, and selec t the \\nmost adaptive individuals to enhance LLM to \\ngenerate accurate and comprehensive news \\nsummaries . \\nExperiments have proven that news \\nsummary generators can generate accurate and \\ncomprehensive news summaries from news \\ntexts , providing concise and accurat e natural \\nlanguage descriptions to better understand and \\nconvey the core content of events, and are \\nvaluable for intelligence analysis.  \\n\\n'),\n",
       " Document(metadata={'source': '3'}, page_content='2. Background and Related Work \\nThis section reviews related research on news \\nsummary generation , event extraction , and \\nknow ledge acquisition using LLM . Genetic \\nalgorithms are also introduced.  \\n2.1 Research on Event Extraction  \\nEvent extraction is an important task in the \\nfield of natural language processing, aiming at \\nextracting key information about events from \\ntext. The event extraction task can be divided \\ninto two main areas:  \\n1. Trigger Identification: The goal of this \\ntask is to identify keywords or phrases in the \\ntext that indicate the occurrence of an event, \\nalso known as event trigger words. Trigger \\nwords are usually verbs  or noun phrases that \\ncan explicitly or implicitly indicate the \\noccurrence of an event. Identifying trigger \\nwords is the first step in event extraction and \\nhelps to identify the events present in the text \\n[14]. \\n2. Event Argument Identification and \\nClassifi cation: Once the event trigger word is \\nidentified, the next task is to identify the \\narguments associated with the event, i.e., \\ninformation about the participants, time, and \\nplace of the event. The event argument \\nelements can be noun phrases or verb phrases , \\nwhich have relationships with the event trigger \\nwords, such as subject -action, action -object, \\netc. \\nThe event extraction task\\'s challenge is \\ndealing  with semantic complexity, polysemy , \\nand context dependency. A combination of \\ntechniques such as lexical an notation, \\ndependent syntactic analysis, entity recognition , \\nand semantic role annotation is required to \\nidentify event trigger words and event thesis \\nelements and to classify and associate them. \\nEvent extraction is of great value in \\napplications such as in formation extraction, \\ntext understanding , and knowledge graph \\nconstruction, and helps to extract and organize \\nevent information from large -scale text data to support various practical application scenarios.  \\nTraditional machine learning -based  event \\nextracti on tasks can be divided into pipeline -\\nbased event extraction and union -based event \\nextraction according to the solution process of \\nthese above tasks [15]. The pipeline -based \\napproach treats all subtasks as independent \\nclassification problems:  [16, 17] , and the joint -\\nbased approach [ 18, 19 ]. \\nThe set of event types and their meta -roles \\nobtained from event extraction constitutes an \\nevent model that helps to completely describe \\nthe event and understand its aspects , in the form \\nof \"type: bombing ; meta -roles : per petrator, \\nvictim, target, tool\" [20] . \\nThe event schema  consists of two parts: \\nevent type describes the general category or set \\nof categories to which the event belongs , and \\nmeta -roles refer to the semantic roles or \\nfunctions associated with the event in di fferent \\nparts of the event schema. Each theoretical \\nmeta -role represents an element or role in the \\nevent, such as the performer of the action, the \\nbearer of the action, the object of influence of \\nthe action, etc. They describe the relationships \\nand roles b etween different participants in the \\nevent. Argumentative meta -roles are usually \\nabstract and aim to represent universal \\nconcepts or role types in an event . Common \\nmeta -roles include Subject, Object, Time , Place , \\nActions , etc. By defining and identifying \\ndifferent theoretical meta -roles, the \\ncomponents of an event can be more accurately \\ndescribed and the semantic analysis and \\nunderstanding of the event can be performed.  \\nEvent patterns provide a structured way to \\nrepresent and describe events. Through event \\npatterns, we can capture important information \\nabout events, understand and analyze the core \\ncontent, relationships , and characteristics of \\nevents , and identify the key elements and \\nplayers of events.  \\nNatural language generation is based on \\nevent patterns,  i.e., generating corresponding \\n'),\n",
       " Document(metadata={'source': '4'}, page_content=\"natural language descriptions or sentences \\nbased on event patterns to generate specific and \\naccurate representations of events for \\ngenerating news headlines, summaries, reports, \\netc. \\n2.2 Genetic Algorithm  \\nGenetic algorithm [22] is an optimization \\nalgorithm based on the theory of biological \\nevolution, which simulates the evolutionary \\nprocess in nature and searches and optimizes \\nthe solution space of a problem through genetic \\noperations (e.g., selection, crossover, and \\nmutation ). \\nThe basic idea is to create a set of individuals \\n(called a population), each of which represents \\na candidate solution to the problem. By \\niterating generation by generation, the \\nsolutions in the population are improved by \\nselecting good individuals, cros sover and \\nvariation operations, and gradually \\napproximating the optimal solution.  \\nIn this paper, we use LLM to generate \\noriginal populations and evolve individuals by \\nselection and crossover in the following \\nprocess : \\n1. Initial population generation: LLM \\ngenerates event patterns as individuals from \\nevent representations based on contextual fine-\\ntuning  unsupervised , and constructs a pool of \\ncandidate event patterns as the original \\npopulation . \\n2. Adaptation assessment: Each individual is \\nassessed using an ada ptation function that \\nmeasures accuracy and comprehensiveness \\n(metrics) . \\n3. Selection: Based on the evaluation results \\nof the fitness function, the individual with \\nhigher fitness is selected as the parent solution \\nand used to generate the next-generation  \\nsolution.  \\n4. Crossover : The gene crossover operation \\nis performed on the newly generated solutions \\nto increase the exploitability  of the solution \\nspace by introducing randomness.  5. Update the population: replace or merge \\nthe newly generated individuals with the parent \\npopulation to form an updated pool of event \\npatterns . \\n6. Repeat steps 2 to 5: iteratively perform \\nselection and crossover operations until the \\ntermination conditions are met .  \\nAfter the evolutionary process of the genetic \\nalgorithm, the event patterns with the highest \\nfitness are filtered out in the pool of candidate \\nevent patterns. These event patterns have high \\nfitness values and quality to summarize the \\ntext's important events and key information . \\n2.3 Text Summary Generation  \\nNatural language  expressions of events are \\ndiverse and sparse, and summarizing news \\nsummaries , i.e., unifying different event \\nrepresentations and presenting the core content \\nof events with concise and accurate natural \\nlanguage descriptions, is of great value for \\nintellige nce analysis. Summarizing text can \\nhelp one quickly understand and organize large \\namounts of intelligence data, extract key \\ninformation, avoid drowning in noise, and thus \\nunderstand and convey the main points of an \\nevent more effectively. This un ified and \\nsummarized description helps eliminate \\ninformation redundancy and ambiguity, \\nimproves the efficiency and accuracy of \\nintelligence analysis, and provides strong \\nsupport for decision -making.  \\nBy summarizing news summaries, a large \\namount of heteroge neous information can be \\nbetter processed and utilized to provide \\npowerful guidance and decision support for \\nintelligence work. Often, intelligence analysis \\nwithin niche domains has similar needs, \\nrequiring the abstraction of a unified summary  \\nfrom a large  number of similar events.  \\nTraditional news summaries are generated \\nusing rules and templates designed by  human \\nexperts  to generate news summaries and \\nprobability -based methods, including the use of \\ntext clustering to group related events into pre -\\n\"),\n",
       " Document(metadata={'source': '5'}, page_content='defined summaries based on manually edited \\nsummaries, in addition to methods that use the \\nlongest common subsequence to generate a \\nrepresentation of news summaries [11] but are \\nmore affected by data sparsity . \\nWith the development of deep learning \\ntechniques, neura l network -based approaches \\n[23] have gradually become the mainstream of \\nnews summary generation, which can better \\nhandle the semantic and contextual complexity \\nand overcome the problem of data sparsity, but \\noften lead to overfitting due to the \\nunavailabili ty of large -scale training data [24]. \\nWith the rise of LLM , they have shown good \\nresults on many natural language processing \\ntasks, but the black -box nature of LLM  causes \\na lack of interpretability in the results they \\ngenerate, which makes it difficult to gain \\ninsight into how the models generate news \\nsummaries from the input text data.  \\nThe interpretation of event patterns can also \\nprovide a basis for evaluating and interpreting \\nthe generated news summaries. Using event \\npatterns to explain the generation of  news \\nsummaries can help us understand how LLM  \\ngenerates  high-quality news summaries based \\non event -related information , and we can gain \\na clearer understanding of how the models \\nunderstand and represent key features, \\nsemantic relationships, and contextual  \\ninformation about events.  \\n2.4 Knowledge Acquisition with LLM  \\nThe advantage of using LLM  for knowledge \\nacquisition lies in their ability to understand \\nand generate language and to learn and extract \\nknowledge from large -scale text data . \\nCao [ 21] et al. prop osed five cycles of \\nknowledge in LLM: acquisition, representation, \\nexploration, editing, and application.  \\nIn the field of news summary generation, \\nLLM  can generate accurate and concise text \\nsummaries by understanding and semantic \\nanalysis of text. It can h elp extract the key \\ninformation and core content in the text, thus enabling knowledge distillation and acquisition. \\nHowever, it should be noted that LLM  also has \\nsome limitations, such as the quality of the \\ngenerated news summary representations , and \\nchall enges such as accuracy and coverage.  \\nTo improve the quality of LLM -generated \\nnews summary representations and to address \\nthe challenges of accuracy and \\ncomprehensiveness, this paper proposes an \\nLLM -based news summary generation \\nparadigm and designs a news summary \\ngenerator based on it to generate high -quality, \\naccurate, and comprehensive news summary \\nrepresentations from a large number of event \\nrepresentations.  \\n3. News Summary  Generation  \\nThis section describes the method of news \\nsummary generation using the large language \\nmodel.  \\nSince a piece of news text often has a large \\namount of data, direct generation of event \\npatterns may lead to the introduction of noise \\nand phantom data due to too many \\nargumentative roles and may cause important \\nargumentative elements t o be missing, so we \\nextract different patterns from the different \\nevent contents contained in it and use a genetic \\nalgorithm to evolve the extracted event patterns \\nand evaluate the final patterns by the fitness \\nfunction, and the pattern with the highest sc ore \\nThe highest scoring patterns are input to LLM \\nto generate news summaries . \\nFormally, given the corpus and LLM, there  \\nare N news  fragments in the corpus { T 1, T2, ..., \\nTN }, each news fragment can extract multiple \\nevents , LLM obtains an event pattern from \\neach event in the news fragment Tn, n ∈ N \\nand constructs a pool of event patterns 𝑝𝑛 as \\nthe original population for evolution , 𝑝𝑛 \\n={𝑠𝑛1 , 𝑠𝑛2 , ... ,𝑠𝑛𝑖 ..𝑠𝑛𝑖  represents the i -th event \\npattern in the n -th event pattern pool . \\nIn this paper, we use a genetic algo rithm to \\nevolve the population and select the crossover \\n'),\n",
       " Document(metadata={'source': '6'}, page_content=\"operations of the theoretical roles contained in \\nthe individuals . Based on the above method , we \\ndesign the News Summary Generator (NSG ) to enhance the LLM news summary generation, \\nand the framework is  shown in Figure 2. \\nFigure 2 The Framework of NSG  \\n \\nThe News Digest Generator contains three \\ncomponents:  \\n1. Candidate event pattern pool generation.  \\n2. Event pattern population evolution.  \\n3. News summary generation . \\nIn the following , we will describe t hese \\nsections in detail.  \\n3.1 Candidate Event Pattern Pool \\nGeneration  \\nEvent patterns are the basis for generating news \\nsummaries, providing abstraction and \\ngeneralization of event representations.  By \\nacquiring event patterns, key information, \\nthemes , and co re contents can be extracted \\nfrom a large number of event representations, \\nthus enabling efficient understanding and \\nsummarization of events.  \\nReal-world text data usually contains noise \\nand diversity, including misspellings, irregular \\ngrammar, randomness, etc. These factors \\nincrease the difficulty of accurately extracting event patterns from text, requiring models with \\ncertain robustness and generalization \\ncapabilities. Event patterns often exist in the \\nimplicit information of text rather than explicit \\nrepresentations , and the model needs to be able \\nto understand contextual dependencies and \\ninfer the implicit event information in order to \\ncapture event patterns accurately. Traditional \\nneural network -based event pattern extraction \\nmethods to acquire event pat terns require a \\nlarge amount of training data, and in some \\ndomains or specific event types, relevant data \\nmay be very limited , and this data scarcity can \\nlimit the model's ability to learn and understand \\nevent patterns.   \\nLLM is equipped with powerful langu age \\ngeneration and context learning capabilities \\nthrough the training of large -scale text data. \\nBased on this, this paper uses LLM to \\nunsupervised  and automatically generate \\ncandidate event patterns from the original event \\n\\n\"),\n",
       " Document(metadata={'source': '7'}, page_content='representations.  \\nBased on this fr amework, we use LLM for \\ncontextual fine-tuning . First let LLM select an \\noverall event type for the news to generalize, \\nthen identify different events from the text and \\nget as many event patterns as we need based on \\neach event. The purpose of context fine -tuning \\nis to improve the performance and \\ngeneralization of the model on a specific task \\nand to generate results as we need them . The \\nadvantage of context fine -tuning is that it can \\ntake advantage of the general language \\nknowledge already learned by the pre -trained \\nlanguage model and fine -tune it on a specific \\ntask, and by fine -tuning the model on task -\\nrelevant data, it allows the model to better \\nunderstand the task -specific semantics and \\ncontext and to generate task -relevant output.  \\nEach context is a text pa ttern pair containing \\nan event representation and the event patterns \\nobtained from that event representation, which \\nis used to guide the LLM  for a generation. LLM \\nunsupervised  acquires event patterns from \\ndifferent types of events based on contextual \\nfine-tuning, and constructs a pool of N event \\npatterns based on N news segments in the \\ncorpus, with each event pattern pool evolving \\nas a primitive population . \\n3.2 Knowledge Evolution  \\nIn this paper, we use the idea of genetic \\nalgorithm to generate news summarie s by \\ncombining good genomes between individuals \\nof different parents through genetic \\nrecombination to produce offspring with higher \\nadaptability , select crossover by using the \\ntheoretical meta -role in event patterns as genes, \\nand finally select the event p attern with the \\nhighest adaptability from the population . The \\npool of event patterns generated in the previous \\nsection provides the raw material for the \\nevolution of event pattern selection.  \\nIn this paper , a pool of N event patterns \\nobtained from N news cl ips is used as the \\noriginal population , and the event patterns in the event pattern pool 𝑝𝑛  corresponding to \\nnews clips Tn, n ∈ N are used as \\nchromosomes , which are formally the set of \\nevent types and argument roles : {Type; \\nArguments. . In each generation, all \\nchromosomes in the population need to be \\nevaluated using the fitness function. \\nChromosomes with higher fitness values are \\nplaced in the mating pool, replacing \\nchromosomes with lower fitness to update the \\npopulation . \\nIn this paper, we evaluate the salience and \\nreliability of the thesis  meta -role in terms of \\nfrequency of occurrence and i mportance , \\nrespectively, and design the fitness function. \\nThe salience and reliability of the thesis meta -\\nrole determine the accuracy and \\ncomprehensiveness of the generated summary.  \\nThe TF -IDF score is the event pattern 𝑠𝑛𝑖 \\nratio of the frequency of occurrence in the \\ncurrent population to the frequency of \\noccurrence in the full population. The formula \\nis as follows:  \\nF(𝑠𝑛𝑖)=(1+log (freq (𝑠𝑛𝑖))2)∗\\nlog(𝑁\\n∑𝑓𝑟𝑒𝑞 (𝑠𝑛𝑖)𝑁𝑛)                     (1) \\nfreq (𝑠𝑛𝑖) for the thesis meta -role 𝑠𝑛𝑖 \\nfrequency of occurrence in P n. \\nThe TextRank [25] score treats a document \\nas a network of words and the links in the \\nnetwork as semantic relationships between the \\ntheoretical roles, with the follo wing equation:  \\nW(𝑠𝑛𝑖)=(1−𝑑)+𝑑∗∑𝑠𝑛𝑗∈\\n𝐼𝑛(𝑠𝑛𝑖)𝑤𝑖𝑗\\n∑𝑠𝑛𝑘∈𝑂𝑢𝑡(𝑠𝑛𝑗)𝑤𝑗𝑘∗𝑊(𝑠𝑛𝑗)        (2) \\nThe fitness function Q is defined as the \\nweighting of the TFIDF score and TextRank \\nscore of the argume ntative meta -role in the \\nevent pattern.  The formula is as follows:  \\n             Q=α∗F(𝑠𝑛𝑖)+β∗W(𝑠𝑛𝑖)       (3) \\nα and β is are the hyperparameters , and d \\nis the damping factor.  \\nChromosomes with the highest scores are \\nconsidered to be the best c urrent solution. The \\n'),\n",
       " Document(metadata={'source': '8'}, page_content='selection of chromosomes with higher fitness \\nallows the genetic algorithm to select the \\nhighest -quality  event patterns.  \\nFor the selection of parents, the fitness of \\neach event pattern in the population was first \\nevaluated according to Equation ( 3), and a \\nportion of individuals were selected as parents \\nto participate in the next evolution according to \\nthe fitness value using the roulette algorithm \\n[26], where event patterns with higher fitness \\nhave a higher probability of being selected,  \\nwhile event patterns with lower fitness still \\nhave a certain probability of being selected, \\nwhich is conducive to maintaining the diversity \\nof the population . \\nSince the event patterns contain fewer and \\nless diverse argument roles, introducing \\nvariation op erations at this time may introduce \\nunnecessary randomness , so using only \\ncrossover operations here can maintain the \\nstability of the population and allow faster \\nconvergence to the optimal solution. The \\ncrossover operation combines the \\nchromosomes in the p ool randomly two by two \\ninto event pattern pairs and swaps its own k \\nargument roles , k being a random number no \\ngreater than the minimum number of argument \\nroles contained in the two event patterns . \\nSubsequently, the progeny individuals \\nobtained by crossov er are merged with the \\nparent individuals to form a new generation of \\npopulation. The new generation population is \\ninvolved in the selection and crossover \\noperation as the parent of the next iteration. In \\nthis paper, we set the maximum number of \\niterations  I, I am the hyperparameter , and the \\nalgorithm stops running when this number is \\nreached, and finally , the event patterns in the \\npopulation are ranked according to the fitness \\nvalue, and the individual with the highest \\nfitness value is selected to represen t the most \\naccurate and comprehensive event pattern with \\nthe highest quality as a hint for news summary \\ngeneration.  3.3 News Summary Generation  \\nWe use 3.2 to obtain highly adaptable event \\npatterns and input the highly adaptable event \\npatterns into the LLM  to generate more \\naccurate and comprehensive news summaries . \\nThe black -box characteristics  of the LLM  also \\ncause  the generated results to often lack \\ninterpretability. The event model can provide \\nexplanations for the news summaries  generated \\nby the LLM , thus  increasing their \\ninterpretability and making the generated \\nresults easier to understand and interpret.  \\nIn the second step, we extract a large number \\nof event patterns from the event representations \\ncontained in each news clip and evolve the \\npopulation con tinuously, and after a certain \\nnumber of evolutions sort the event pattern \\nwith the highest fitness s as the high-quality  \\nevent pattern. n news clips are obtained with a \\ntotal of N high-quality  event patterns  S = \\n{s1 ,s2 ,...,sN }. The set of high -quality event \\npatterns is then input into the large language \\nmodel to obtain news summaries . \\nThe quality of the generated news \\nsummaries is assessed by a series of metrics \\nsuch as ROUGE, BLEU , and Overlap scores.  \\nBy combining high -quality event patterns \\nwith the generative power of large language \\nmodels, we are able to obtain more accurate \\nand comprehensible news summaries , resulting \\nin better results in terms of information \\nconveyance and understanding. This approach \\nnot only improves the quality of news \\nsummaries but also enhances our \\nunderstanding of the news summary generation \\nprocess.  \\n4. Experiment  \\n4.1 Experimental Preparation  \\nDataset : \\nWe experimented with the PENS [27] news \\nheadline generation dataset , which contains \\n113,762 news articles in 15 topic typ es, each \\ncontaining id, headline, content and a category \\n'),\n",
       " Document(metadata={'source': '9'}, page_content='tag, of which we only used the news headline \\nand content . The grain storage pest event \\ncorpus constructed by Xiao Le [ 28] et al. is also \\nused as a niche domain dataset for comparison \\nto explore the reliability of the summaries \\ngenerated by the news summary generator \\nproposed in this paper on a niche specialized \\ndomain, which is a grain storage pest event \\ncorpus including event types and event texts \\nobtained from grain storage pest texts after \\nevent ex traction.  \\nLLM : \\nWe use ChatGLM [29] as the backbone large \\nlanguage model of our news summarization \\nframework.  \\nThe effectiveness of the method proposed in \\nthis paper was verified by experimenting with \\ntwo different tasks:  \\n(1) The event pattern getter acquire s and \\nevolves the event pattern quality enhancement \\neffect afterward . \\n(2) The experimental results of the \\nframework on the niche domain dataset are \\ncompared with the PENS news headline \\ngeneration  dataset to demonstrate the \\ngeneralization ability of the obt ained news \\nsummaries.  \\n4.2 Results of News summary generator  \\nIn this section, the quality of news summaries \\ngenerated by NSG is evaluated, and the \\nbaseline selection of the TFIDF -based algorithm and TextRank -based algorithm for \\nnews summary generation. As w ell as a Gpt2 -\\nbased news summary generation method [ 30], \\nthe main idea is to generate multiple summaries \\nand select the most appropriate news summary \\nfrom them, while we use LLM to generate \\nsummaries directly on the text.  \\nExperimental evaluation metrics ar e used \\nROUGE, BLEU and overlap score, ROUGE is \\na text summarization evaluation metric based \\non n -gram and recall, which measures the \\nquality of summarization by examining aspects \\nsuch as word overlap, sentence -level similarity , \\nand sequence -level similarit y in summarization , \\nROUGE [31] metric system includes several \\nevaluation metrics, such as  ROUGE -1, \\nROUGE -2, and ROUGE -L, etc., which denote \\nmeasuring one -word matching, measuring \\nbinary word matching, and recording the \\nlongest common subsequence, respectiv ely, \\nand are widely used in the evaluation and \\ncomparison of text summarization tasks. BLEU \\n[32] is a precision -based similarity measure, \\nand the common metrics are BLEU -1, BLEU -\\n2, BLEU -3, and BLEU -4, Overlap  detects the \\nfrequency of occurrence of repeated  phrases \\nbetween the generated news headlines and the \\nreference headlines , and can measure to what \\nextent the model is copying phrases directly \\nfrom the text as summaries . \\n  ROUGE    BLEU     \\nModel  R-1 R-2 R-L B-1 B-2 B-3 B-4 Overlap  \\nTFIDF  0.265 0.087 0.183 0.241 0.132 0.129 0.048 59 \\nTextRank  0.354 0.143 0.282 0.312 0.188 0.174 0.082 52 \\nGpt2  0.398 0.182 0.357 0.39 0.261 0.208 0.117 42 \\nGLM  0.489 0.197  0.382  0.414  0.298  0.236  0.142  45 \\nGLM+NSG  0.568  0.224  0.403  0.433  0.315  0.26 0.174  43 \\nTable 1: Compari son of experimental results of GLM+NSG with baseline methods. We chose \\nROUGE , BLEU and Overlap  as evaluation metrics, and the values of Overlap are percentage s \\n \\nWe compare the news summary generation \\nmethod of LLM plus NSG proposed in this paper with the above baseline most, the effect \\nof LLM plus NSG for news summary \\n'),\n",
       " Document(metadata={'source': '10'}, page_content='generation is 0.431 and 0.417 higher than \\nTFIDF and TextRank, respectively, which \\nindicates that the method of TFIDF and \\nPageRank weighting the adaptation of \\nargumentative roles and then gene rating news \\nsummaries is better than directly using TFIDF \\nand PageRank is better.  \\nAlso compared with Gpt2 and direct \\nsummary generation with a large model, the \\neffects were 0.164 and 0.079 higher, \\nrespectively, demonstrating the effectiveness of \\nextracting  event patterns before population \\nevolution.  4.3 Results in the Professional Field \\nThe experimental effect on the grain storage \\ndomain dataset , the grain storage pest dataset is \\nconstructed for the purpose of constructing a \\ngrain storage pest matter mappin g, and the \\nevent text paragraphs are short, and the data are \\nin the form of grain storage pest related events \\nand the corresponding matter , which is the \\nabstract generalization of the corresponding \\nevent, which we use as the event title for the \\nexperiment . \\n  ROUGE    BLEU     \\nModel  R-1 R-2 R-L B-1 B-2 B-3 B-4 Overlap  \\nTFIDF  0.195 0.094 0.122 0.062  0.105 0.116 0.052 63 \\nTextRank  0.221 0.113 0.168 0.274 0.131 0.134 0.063 54 \\nGpt2  0.256  0.127  0.219 0.241 0.163 0.152 0.072 38 \\nGLM  0.314 0.133 0.273 0.284 0.179 0.163 0.088 40 \\nGLM+ NSG  0.378 0.172 0.305 0.312  0.202  0.193 0.112  42 \\nTable 2: GLM+ NSG  results on stored grain pest dataset compared to baseline experiments  \\n \\nThe experimental results on the corpus of \\nstored grain pests we can see that the effect of \\nthe L LM plus NSG  approach decreases on the \\nPENS dataset, but the overall effect is still \\nbetter than the baseline, which proves the \\ngeneralization of the news summary extraction \\nparadigm proposed in this paper . On the \\nOverlap  metric, since the methods based on \\nTFIDF and TextRank are extractive methods, \\nthey are more dependent on the original data \\nwhen the amount of data is small, so the \\nobtained event summaries have a higher \\nOverlap  rate with the original text, while the \\nmethods based on GPT2 and LLM and the \\nproposed method in this paper are generative, \\nso the Overlap  rate is reduced.  \\nExperimental results show that the proposed \\nmethod in this paper still achieves good results \\non the grain storage pest dataset, proving to be \\nequally effective for summary generatio n tasks \\nin niche domains . 4. Conclusion  \\nIn this paper, we propose a new paradigm for \\nnews summary generation and design a News \\nSummary  Generator  (NSG) . Experimental \\nverif y that the proposed approach in this paper \\nis able to generate accurate and reliable news  \\nsummaries on news summary generation tasks \\nwith certain generalizations  and application to \\nrelated tasks. At the same time, by introducing \\nthe event model into the news summary \\ngeneration process of the LLM , the problem of \\nits lack of interpretability can  be solved to a \\ncertain extent, enabling us to understand and \\nexplain the internal operation of the model  \\nbetter , as well as the basis and logic of \\ngenerating news summaries. This helps to \\nimprove the credibility and reliability of the \\nLLM  and provide more  reliable and \\ninterpretable results for the news summary \\ngeneration task. \\n'),\n",
       " Document(metadata={'source': '11'}, page_content=' \\n \\n \\n \\nReferen ce: \\n[1] Devlin J, Chang M W, Lee K,  et al.BERT: \\nPre-training of Deep Bidirectional \\nTransformers for Language Understanding[J].  \\n2018 , pages  4171 –4186  \\n[2] Lewis  M, Liu  Y, Goyal N, et al.  BART: \\nDenoising sequence -to-sequence pretraining \\nfor natural language generation, translation,  \\nand comprehension [C]// In Proc. of ACL, \\npages 7871 –7880  \\n[3] Brown T B, Mann B, Ryder N,  et al.  \\nLanguage Models are Few -Shot Learners[J].  \\n2020 , NeurIPS  2020,  pages 6 -12 \\n[4] OpenAI. ChatGPT: Optimizing language \\nmodels for dialogue, 2022.  \\n[5] OpenAI. GPT -4 technical report. arXiv \\npreprint arXiv:2303.08774, 2023.  \\n[6] Scao T, Fan A, Akiki C, et al. BLOOM: A \\n176B -Parameter Open -Access Multilingual \\nLangua ge Model [J]. 2023 , ArXiv preprint, \\nabs/2211.05100.  \\n[7] Vaswani A, Ahazeer N, Parmar N, et al. \\nAttention is All You Need [C]// In Advances in \\nNeural Information Processing Systems 30: \\nAnnual Conference on Neural  Information \\nProcessing Systems 2017 , pages  5998-6008  \\n[8] Wei X, Cui X, Cheng N, et al. Zero -Shot \\nInformation Extraction via Chatting with \\nChatGPT  [J]. arXiv:2302.10205 , 2023  \\n[9] Kıcıman  E, Ness R, Sharma A, et al. Causal \\nReasoning and Large Language Mod els: \\nOpening a New Frontier for Causality [J]. \\narXiv:2305.00050 ,2023  \\n[10] Wang M, Tanaka H, Zhong Y .1 \\nGenerating Summaries of Multiple Technical \\nArticles[J].  2023  \\n[11] Wang W, Shi C, Yu X, et al. An extracti ve \\ntopic brief representation generation method to \\nevent [J]. Journal of Shandong University  \\n(Natural Science) , 2021,56(05):66 -75+84.  [12] Han Y, Xu X, Li B , et al.  Web News Multi -\\ndocument Summarization Based on Event \\nExtraction [J]. Journal of Chinese Infor mation \\nProcessing, 2012,26(01):58 -66. \\n[13] Atapattu T, Falkner K. A Framework for \\nTopic Generation and Labeling from MOOC \\nDiscussions [J]. Proceedings of the Third ACM \\nConference on Learning , 2016, pages 201 -204  \\n[14] Hong Y, Zhang J, Ma B,  et al.Using Cross -\\nEntity Inference to Improve Event \\nExtraction[C]//  Meeting of the Association for \\nComputational Linguistics:  Human Language \\nTechnologies.  Association for Computational \\nLinguistics, 2011.  \\n[15] LI Q,  LI J,  SHENG J,  et al. A Compact \\nSurvey on Event Extraction: Approaches and \\nApplications[J]. arXiv:2107.02126 , 2021  \\n[16] CHEN Y,  XU L,  LIU K,  et al. Event \\nExtraction via Dynamic Multi -Pooling \\nConvolutional Neural Networks [C]// \\nInternational Joint Conference on Natural \\nLanguage Processing,  2015 , pages 167-176 \\n[17] SUBBURATHINAM A,LU D, JI H,  et al. \\nCross -lingual structure transfer for relation and \\nevent[C]// International Joint Conference on \\nNatural Language Processing , 2019 , pages \\n313-325 \\n[18] ZHANG J,  QIN Y ,  ZHANG Y ,  et al.  \\nExtracting entities and events as a single task \\nusing a transition -based neural model[C]//  \\nInternational Joint Conference on Artif icial \\nIntelligence,2019 , pages 5422 -5428  \\n[19] LI D,HUANG L,JI H,  et al. Biomedical \\nevent extraction based on knowledge -driven \\ntree-lstm[C]//  Human  Language  Technologies , \\n2019 , pages 1421 -1430  \\n[20] Chambers N, Jurafsky D. Template -based \\ninformation extraction without  the templates. \\n[C]//  In Proceedings of the 49th Annual  \\nMeeting of the Association for Computational  \\nLinguistics: Human Language Technologies, \\n2011, pages  976–986 \\n'),\n",
       " Document(metadata={'source': '12'}, page_content='[21] Cao B, Lin H, Han X, e t al. The Life Cycle \\nof Knowledge in Big Language Models: A \\nSurvey [J]. arXiv:2307.01189 , 2023  \\n[22] Goldberg D. E., Holland J. H. Genetic \\nalgorithms and machine learning. machine \\nlearning [J], Machine Learning , 1988, 3(2), \\npages 95-99 \\n[23] Gehring J, Auli M, Grangier D,  et al. \\nConvolutional sequence to sequence \\nlearning [J]. Proceedings of the 34th \\nInternational Conference on Machine Learning, \\n2017, pages 1243 -1252  \\n[24] Li J, Tang T, Zhao W, et al. Pretrained \\nLanguage Models for Text Generation: A \\nSurvey [J], arXiv:2201.05273 , 2022  \\n[25] R. Mihalcea and P. Tarau, TextRank: \\nBringing order into text, in Proc. 2004 \\nConference  on Empirical Methods in Natural \\nLanguage Processing, 2004,  pages  404–411 \\n[26] Lipowski A, Lipowska D.  Roulette -wheel \\nselection via stochastic acceptance[J]. Physica \\nA Statistical Mechanics & Its Applications, \\n2012, 391(6):2193 -2196.   \\n[27] Ao X, Wang X, Luo L, et al. PENS: A \\nDataset and Generic Framework for \\nPersonalized News Headline Generation [C]// \\nThe Annual Meeting of  the Association for \\nComputational Linguistics (ACL) , 2021  \\n[28] Xiao L , Chen X, Shan X.  Construction of \\nStored Grain Pest Event Evolutionary Graph  \\n[J]. Journal of the Chinese Cereals and Oils \\nAssociation , 2022  \\n[29] Du Z, Qian Y, Liu X, et al. GLM: General  \\nLanguage Model Pretraining with \\nAutoregressive Blank Infilling [J]. \\narXiv:2103.10360 , 2023  \\n[30] Mishra P, Diwan C, Srinivasa s , et al. \\nAutomatic Title Generation for Text with Pre -\\ntrained Transformer Languag e Model [C]// \\n2021 IEEE 15th International Conference on \\nSemantic Computing (ICSC), 2021, pages 17 -\\n24 \\n[31] Lin C. ROUGE: A Package for Automatic \\nEvaluation of Summa ries[C]// Post-Conference Workshop of ACL 2004  \\n[32] Papineni K , Roukos S , Ward T , et al. \\nBLEU: a Method for Automatic Evaluation of \\nMachine Translation [C]//  Proceedings of the \\n40th Annual Meeting on Association for \\nComputational Linguistics , 2002 , pages 311 -\\n318 \\n \\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ' \\nEnhancing LLM with Evolutionary Fine -Tuning for News \\nSummary Generation  \\nLe Xiao, Xiaolin Chen  \\nCollege of Information Science and Engineering, Henan University of Technology, \\nZhengzhou, China  \\nxiaole@haut.edu.cn chenxiaolin @stu.haut.edu.cn  \\nAbstract  \\nNew s summary generation is an \\nimportant task in the field of intelligence \\nanalysis, which can provide accurate and \\ncomprehensive information to help \\npeople better understand and respond to \\ncomplex real -world events. However, \\ntraditional news summary generatio n \\nmethods face some challenges, which are \\nlimited by the model itself and the \\namount of training data, as well as the \\ninfluence of text noise, making it difficult \\nto generate reliable information \\naccurately. In this paper, we propose a \\nnew paradigm for new s summary \\ngeneration using LLM  with powerful \\nnatural language understanding and \\ngenerative capabilities. We use LLM  to \\nextract multiple structured event patterns \\nfrom the events contained in news \\nparagraphs, evolve the event pattern \\npopulation with genetic  algorithm, and \\nselect the most adaptive event pattern to \\ninput into the LLM  to generate news \\nsummaries. A News Summary Generator  \\n(NSG ) is designed to select and evolve \\nthe event pattern populations and \\ngenerate news summaries. The \\nexperimental results sho w that the news \\nsummary generator is able to generate \\naccurate and reliable news summaries \\nwith some generalization ability.  \\n 1. Introduction  \\nIn recent years, pre -trained language models \\nhave undergone rapid development [1, 2, 3, 4, \\n5, 6], especially models b ased on the \\nTransformer [7] architecture, which has the \\nability to process natural language. These \\nmodels are able to automatically learn \\nstatistical patterns and patterns in language by \\ntraining on large -scale textual data , which \\nmakes pre -trained languag e models widely \\nadaptable and can be applied to a variety of \\ndomains and tasks.  \\nLarge language m odels  (LLM)  have \\nimproved the experimental results of many \\nnatural language processing tasks, exceeding \\nthe previous state -of-the-art of deep learning \\nmodels in  tasks such as information extraction \\n[8] and causal inference [9], and therefore, how \\nto enhance the performance of LLMs in \\nspecific tasks has attracted extensive research.  \\nNews summary generation is a type of \\ndocument summary generation [10], which \\naims to generate concise and important event \\ntopics in a paragraph of text to better \\ncommunicate intelligent  content. It plays a key \\nrole in areas such as information processing, \\nintelligence analysis, research , and decision -\\nmaking . Automatic news summary gener ation \\ncan provide accurate and comprehensive \\ninformation to help people better understand \\nand respond to complex real -world events.  \\nTraditional news summary generation is \\nmainly used to generate headlines , a task that \\nrequires the model to be able to under stand the \\n',\n",
       " '2': 'key information of the news text and be able to \\nexpress it in a concise manner.  \\nPrevious news headline generation methods \\n[11, 12, 13] currently have some challenges, \\ndue to the limitations of the model itself and the \\namount of training data, it is often difficult for \\nthe model to fully understand the semantics of \\nthe article, and if there is too much noise in the \\nrepresentation of events and the quality of the \\ndata set is low, it may affect the consistency of \\nthe generated headlines with the orig inal text, \\nand it is often difficult to accurately generate \\nreliable headlines for articles in certain \\nspecialized fields , event pattern as a kind of \\nstructured data with concise and accurate \\ncharacteristics . To address the above \\nchallenges, we use LLM to generate news \\nsummaries. News paragraphs often have long \\ntext segments and contain multiple events, each \\nwith an implied event pattern, as in Figure 1, to \\nenhance the summary generation capability of \\nLLM through event pattern evolution.  \\n \\nFigure 1:  Get eve nt patterns from news \\nparagraphs  \\n \\nWe propose a new paradigm for news \\nsummary generation - using LLM  to obtain \\nevent patterns from texts to construct a pool of \\nevent patterns , and with the help of the idea of \\ngenetic algorithm, we use the event patterns as \\nchromosomes, select crossover of \\nchromosomes to evolve the pool of event \\npatterns, and evaluate the chromosomes in the \\npool by the fitness function The highest fitness \\nmeans the best quality, and the best quality individuals are selected to enhance LLM so that \\nLLM generates accurate, reliable and \\ncomprehensive news summaries . \\nTo this end, we designed a news summary \\ngenerator (NSG) to automatically generate \\naccurate and comprehensive news summaries \\nin a three -step process:  \\n1. Unsupervised candidate event pat tern pool \\ngeneration. In the unsupervised case, multiple \\nevents are extracted from the news text and a \\npool of event patterns is constructed based on \\ncontextual fine -tuning of the LLM.  \\n2. Candidate event pattern population \\nevolution.  A genetic algorithm is  used to evolve \\nthe event pattern pool by selecting crossover \\nfor event patterns.  \\n3. Fine -tuning enhanced LLM to generate \\nnews summaries . The most adapted event \\npattern is selected to generate news summaries \\nusing LLM.  \\nMain contributions to this article:  \\nWe propose a new news summary generation \\nparadigm , i.e., with evolutionary knowledge, \\nfine-tuning LLM for enhancement to improve \\nthe news summary generation capability of \\nLLM and effectively address the challenges of \\ninaccurate , incomplete , and poorly \\ninterpretable news summaries generated by \\nLLM.  \\nDesign a news summary generator to \\ngenerate candidate event pattern pools from \\nopen -world  knowledge using LLM, iteratively \\nupdate the event pattern individuals to evolve \\nthe candidate event pattern pools, and selec t the \\nmost adaptive individuals to enhance LLM to \\ngenerate accurate and comprehensive news \\nsummaries . \\nExperiments have proven that news \\nsummary generators can generate accurate and \\ncomprehensive news summaries from news \\ntexts , providing concise and accurat e natural \\nlanguage descriptions to better understand and \\nconvey the core content of events, and are \\nvaluable for intelligence analysis.  \\n\\n',\n",
       " '3': '2. Background and Related Work \\nThis section reviews related research on news \\nsummary generation , event extraction , and \\nknow ledge acquisition using LLM . Genetic \\nalgorithms are also introduced.  \\n2.1 Research on Event Extraction  \\nEvent extraction is an important task in the \\nfield of natural language processing, aiming at \\nextracting key information about events from \\ntext. The event extraction task can be divided \\ninto two main areas:  \\n1. Trigger Identification: The goal of this \\ntask is to identify keywords or phrases in the \\ntext that indicate the occurrence of an event, \\nalso known as event trigger words. Trigger \\nwords are usually verbs  or noun phrases that \\ncan explicitly or implicitly indicate the \\noccurrence of an event. Identifying trigger \\nwords is the first step in event extraction and \\nhelps to identify the events present in the text \\n[14]. \\n2. Event Argument Identification and \\nClassifi cation: Once the event trigger word is \\nidentified, the next task is to identify the \\narguments associated with the event, i.e., \\ninformation about the participants, time, and \\nplace of the event. The event argument \\nelements can be noun phrases or verb phrases , \\nwhich have relationships with the event trigger \\nwords, such as subject -action, action -object, \\netc. \\nThe event extraction task\\'s challenge is \\ndealing  with semantic complexity, polysemy , \\nand context dependency. A combination of \\ntechniques such as lexical an notation, \\ndependent syntactic analysis, entity recognition , \\nand semantic role annotation is required to \\nidentify event trigger words and event thesis \\nelements and to classify and associate them. \\nEvent extraction is of great value in \\napplications such as in formation extraction, \\ntext understanding , and knowledge graph \\nconstruction, and helps to extract and organize \\nevent information from large -scale text data to support various practical application scenarios.  \\nTraditional machine learning -based  event \\nextracti on tasks can be divided into pipeline -\\nbased event extraction and union -based event \\nextraction according to the solution process of \\nthese above tasks [15]. The pipeline -based \\napproach treats all subtasks as independent \\nclassification problems:  [16, 17] , and the joint -\\nbased approach [ 18, 19 ]. \\nThe set of event types and their meta -roles \\nobtained from event extraction constitutes an \\nevent model that helps to completely describe \\nthe event and understand its aspects , in the form \\nof \"type: bombing ; meta -roles : per petrator, \\nvictim, target, tool\" [20] . \\nThe event schema  consists of two parts: \\nevent type describes the general category or set \\nof categories to which the event belongs , and \\nmeta -roles refer to the semantic roles or \\nfunctions associated with the event in di fferent \\nparts of the event schema. Each theoretical \\nmeta -role represents an element or role in the \\nevent, such as the performer of the action, the \\nbearer of the action, the object of influence of \\nthe action, etc. They describe the relationships \\nand roles b etween different participants in the \\nevent. Argumentative meta -roles are usually \\nabstract and aim to represent universal \\nconcepts or role types in an event . Common \\nmeta -roles include Subject, Object, Time , Place , \\nActions , etc. By defining and identifying \\ndifferent theoretical meta -roles, the \\ncomponents of an event can be more accurately \\ndescribed and the semantic analysis and \\nunderstanding of the event can be performed.  \\nEvent patterns provide a structured way to \\nrepresent and describe events. Through event \\npatterns, we can capture important information \\nabout events, understand and analyze the core \\ncontent, relationships , and characteristics of \\nevents , and identify the key elements and \\nplayers of events.  \\nNatural language generation is based on \\nevent patterns,  i.e., generating corresponding \\n',\n",
       " '4': \"natural language descriptions or sentences \\nbased on event patterns to generate specific and \\naccurate representations of events for \\ngenerating news headlines, summaries, reports, \\netc. \\n2.2 Genetic Algorithm  \\nGenetic algorithm [22] is an optimization \\nalgorithm based on the theory of biological \\nevolution, which simulates the evolutionary \\nprocess in nature and searches and optimizes \\nthe solution space of a problem through genetic \\noperations (e.g., selection, crossover, and \\nmutation ). \\nThe basic idea is to create a set of individuals \\n(called a population), each of which represents \\na candidate solution to the problem. By \\niterating generation by generation, the \\nsolutions in the population are improved by \\nselecting good individuals, cros sover and \\nvariation operations, and gradually \\napproximating the optimal solution.  \\nIn this paper, we use LLM to generate \\noriginal populations and evolve individuals by \\nselection and crossover in the following \\nprocess : \\n1. Initial population generation: LLM \\ngenerates event patterns as individuals from \\nevent representations based on contextual fine-\\ntuning  unsupervised , and constructs a pool of \\ncandidate event patterns as the original \\npopulation . \\n2. Adaptation assessment: Each individual is \\nassessed using an ada ptation function that \\nmeasures accuracy and comprehensiveness \\n(metrics) . \\n3. Selection: Based on the evaluation results \\nof the fitness function, the individual with \\nhigher fitness is selected as the parent solution \\nand used to generate the next-generation  \\nsolution.  \\n4. Crossover : The gene crossover operation \\nis performed on the newly generated solutions \\nto increase the exploitability  of the solution \\nspace by introducing randomness.  5. Update the population: replace or merge \\nthe newly generated individuals with the parent \\npopulation to form an updated pool of event \\npatterns . \\n6. Repeat steps 2 to 5: iteratively perform \\nselection and crossover operations until the \\ntermination conditions are met .  \\nAfter the evolutionary process of the genetic \\nalgorithm, the event patterns with the highest \\nfitness are filtered out in the pool of candidate \\nevent patterns. These event patterns have high \\nfitness values and quality to summarize the \\ntext's important events and key information . \\n2.3 Text Summary Generation  \\nNatural language  expressions of events are \\ndiverse and sparse, and summarizing news \\nsummaries , i.e., unifying different event \\nrepresentations and presenting the core content \\nof events with concise and accurate natural \\nlanguage descriptions, is of great value for \\nintellige nce analysis. Summarizing text can \\nhelp one quickly understand and organize large \\namounts of intelligence data, extract key \\ninformation, avoid drowning in noise, and thus \\nunderstand and convey the main points of an \\nevent more effectively. This un ified and \\nsummarized description helps eliminate \\ninformation redundancy and ambiguity, \\nimproves the efficiency and accuracy of \\nintelligence analysis, and provides strong \\nsupport for decision -making.  \\nBy summarizing news summaries, a large \\namount of heteroge neous information can be \\nbetter processed and utilized to provide \\npowerful guidance and decision support for \\nintelligence work. Often, intelligence analysis \\nwithin niche domains has similar needs, \\nrequiring the abstraction of a unified summary  \\nfrom a large  number of similar events.  \\nTraditional news summaries are generated \\nusing rules and templates designed by  human \\nexperts  to generate news summaries and \\nprobability -based methods, including the use of \\ntext clustering to group related events into pre -\\n\",\n",
       " '5': 'defined summaries based on manually edited \\nsummaries, in addition to methods that use the \\nlongest common subsequence to generate a \\nrepresentation of news summaries [11] but are \\nmore affected by data sparsity . \\nWith the development of deep learning \\ntechniques, neura l network -based approaches \\n[23] have gradually become the mainstream of \\nnews summary generation, which can better \\nhandle the semantic and contextual complexity \\nand overcome the problem of data sparsity, but \\noften lead to overfitting due to the \\nunavailabili ty of large -scale training data [24]. \\nWith the rise of LLM , they have shown good \\nresults on many natural language processing \\ntasks, but the black -box nature of LLM  causes \\na lack of interpretability in the results they \\ngenerate, which makes it difficult to gain \\ninsight into how the models generate news \\nsummaries from the input text data.  \\nThe interpretation of event patterns can also \\nprovide a basis for evaluating and interpreting \\nthe generated news summaries. Using event \\npatterns to explain the generation of  news \\nsummaries can help us understand how LLM  \\ngenerates  high-quality news summaries based \\non event -related information , and we can gain \\na clearer understanding of how the models \\nunderstand and represent key features, \\nsemantic relationships, and contextual  \\ninformation about events.  \\n2.4 Knowledge Acquisition with LLM  \\nThe advantage of using LLM  for knowledge \\nacquisition lies in their ability to understand \\nand generate language and to learn and extract \\nknowledge from large -scale text data . \\nCao [ 21] et al. prop osed five cycles of \\nknowledge in LLM: acquisition, representation, \\nexploration, editing, and application.  \\nIn the field of news summary generation, \\nLLM  can generate accurate and concise text \\nsummaries by understanding and semantic \\nanalysis of text. It can h elp extract the key \\ninformation and core content in the text, thus enabling knowledge distillation and acquisition. \\nHowever, it should be noted that LLM  also has \\nsome limitations, such as the quality of the \\ngenerated news summary representations , and \\nchall enges such as accuracy and coverage.  \\nTo improve the quality of LLM -generated \\nnews summary representations and to address \\nthe challenges of accuracy and \\ncomprehensiveness, this paper proposes an \\nLLM -based news summary generation \\nparadigm and designs a news summary \\ngenerator based on it to generate high -quality, \\naccurate, and comprehensive news summary \\nrepresentations from a large number of event \\nrepresentations.  \\n3. News Summary  Generation  \\nThis section describes the method of news \\nsummary generation using the large language \\nmodel.  \\nSince a piece of news text often has a large \\namount of data, direct generation of event \\npatterns may lead to the introduction of noise \\nand phantom data due to too many \\nargumentative roles and may cause important \\nargumentative elements t o be missing, so we \\nextract different patterns from the different \\nevent contents contained in it and use a genetic \\nalgorithm to evolve the extracted event patterns \\nand evaluate the final patterns by the fitness \\nfunction, and the pattern with the highest sc ore \\nThe highest scoring patterns are input to LLM \\nto generate news summaries . \\nFormally, given the corpus and LLM, there  \\nare N news  fragments in the corpus { T 1, T2, ..., \\nTN }, each news fragment can extract multiple \\nevents , LLM obtains an event pattern from \\neach event in the news fragment Tn, n ∈ N \\nand constructs a pool of event patterns 𝑝𝑛 as \\nthe original population for evolution , 𝑝𝑛 \\n={𝑠𝑛1 , 𝑠𝑛2 , ... ,𝑠𝑛𝑖 ..𝑠𝑛𝑖  represents the i -th event \\npattern in the n -th event pattern pool . \\nIn this paper, we use a genetic algo rithm to \\nevolve the population and select the crossover \\n',\n",
       " '6': \"operations of the theoretical roles contained in \\nthe individuals . Based on the above method , we \\ndesign the News Summary Generator (NSG ) to enhance the LLM news summary generation, \\nand the framework is  shown in Figure 2. \\nFigure 2 The Framework of NSG  \\n \\nThe News Digest Generator contains three \\ncomponents:  \\n1. Candidate event pattern pool generation.  \\n2. Event pattern population evolution.  \\n3. News summary generation . \\nIn the following , we will describe t hese \\nsections in detail.  \\n3.1 Candidate Event Pattern Pool \\nGeneration  \\nEvent patterns are the basis for generating news \\nsummaries, providing abstraction and \\ngeneralization of event representations.  By \\nacquiring event patterns, key information, \\nthemes , and co re contents can be extracted \\nfrom a large number of event representations, \\nthus enabling efficient understanding and \\nsummarization of events.  \\nReal-world text data usually contains noise \\nand diversity, including misspellings, irregular \\ngrammar, randomness, etc. These factors \\nincrease the difficulty of accurately extracting event patterns from text, requiring models with \\ncertain robustness and generalization \\ncapabilities. Event patterns often exist in the \\nimplicit information of text rather than explicit \\nrepresentations , and the model needs to be able \\nto understand contextual dependencies and \\ninfer the implicit event information in order to \\ncapture event patterns accurately. Traditional \\nneural network -based event pattern extraction \\nmethods to acquire event pat terns require a \\nlarge amount of training data, and in some \\ndomains or specific event types, relevant data \\nmay be very limited , and this data scarcity can \\nlimit the model's ability to learn and understand \\nevent patterns.   \\nLLM is equipped with powerful langu age \\ngeneration and context learning capabilities \\nthrough the training of large -scale text data. \\nBased on this, this paper uses LLM to \\nunsupervised  and automatically generate \\ncandidate event patterns from the original event \\n\\n\",\n",
       " '7': 'representations.  \\nBased on this fr amework, we use LLM for \\ncontextual fine-tuning . First let LLM select an \\noverall event type for the news to generalize, \\nthen identify different events from the text and \\nget as many event patterns as we need based on \\neach event. The purpose of context fine -tuning \\nis to improve the performance and \\ngeneralization of the model on a specific task \\nand to generate results as we need them . The \\nadvantage of context fine -tuning is that it can \\ntake advantage of the general language \\nknowledge already learned by the pre -trained \\nlanguage model and fine -tune it on a specific \\ntask, and by fine -tuning the model on task -\\nrelevant data, it allows the model to better \\nunderstand the task -specific semantics and \\ncontext and to generate task -relevant output.  \\nEach context is a text pa ttern pair containing \\nan event representation and the event patterns \\nobtained from that event representation, which \\nis used to guide the LLM  for a generation. LLM \\nunsupervised  acquires event patterns from \\ndifferent types of events based on contextual \\nfine-tuning, and constructs a pool of N event \\npatterns based on N news segments in the \\ncorpus, with each event pattern pool evolving \\nas a primitive population . \\n3.2 Knowledge Evolution  \\nIn this paper, we use the idea of genetic \\nalgorithm to generate news summarie s by \\ncombining good genomes between individuals \\nof different parents through genetic \\nrecombination to produce offspring with higher \\nadaptability , select crossover by using the \\ntheoretical meta -role in event patterns as genes, \\nand finally select the event p attern with the \\nhighest adaptability from the population . The \\npool of event patterns generated in the previous \\nsection provides the raw material for the \\nevolution of event pattern selection.  \\nIn this paper , a pool of N event patterns \\nobtained from N news cl ips is used as the \\noriginal population , and the event patterns in the event pattern pool 𝑝𝑛  corresponding to \\nnews clips Tn, n ∈ N are used as \\nchromosomes , which are formally the set of \\nevent types and argument roles : {Type; \\nArguments. . In each generation, all \\nchromosomes in the population need to be \\nevaluated using the fitness function. \\nChromosomes with higher fitness values are \\nplaced in the mating pool, replacing \\nchromosomes with lower fitness to update the \\npopulation . \\nIn this paper, we evaluate the salience and \\nreliability of the thesis  meta -role in terms of \\nfrequency of occurrence and i mportance , \\nrespectively, and design the fitness function. \\nThe salience and reliability of the thesis meta -\\nrole determine the accuracy and \\ncomprehensiveness of the generated summary.  \\nThe TF -IDF score is the event pattern 𝑠𝑛𝑖 \\nratio of the frequency of occurrence in the \\ncurrent population to the frequency of \\noccurrence in the full population. The formula \\nis as follows:  \\nF(𝑠𝑛𝑖)=(1+log (freq (𝑠𝑛𝑖))2)∗\\nlog(𝑁\\n∑𝑓𝑟𝑒𝑞 (𝑠𝑛𝑖)𝑁𝑛)                     (1) \\nfreq (𝑠𝑛𝑖) for the thesis meta -role 𝑠𝑛𝑖 \\nfrequency of occurrence in P n. \\nThe TextRank [25] score treats a document \\nas a network of words and the links in the \\nnetwork as semantic relationships between the \\ntheoretical roles, with the follo wing equation:  \\nW(𝑠𝑛𝑖)=(1−𝑑)+𝑑∗∑𝑠𝑛𝑗∈\\n𝐼𝑛(𝑠𝑛𝑖)𝑤𝑖𝑗\\n∑𝑠𝑛𝑘∈𝑂𝑢𝑡(𝑠𝑛𝑗)𝑤𝑗𝑘∗𝑊(𝑠𝑛𝑗)        (2) \\nThe fitness function Q is defined as the \\nweighting of the TFIDF score and TextRank \\nscore of the argume ntative meta -role in the \\nevent pattern.  The formula is as follows:  \\n             Q=α∗F(𝑠𝑛𝑖)+β∗W(𝑠𝑛𝑖)       (3) \\nα and β is are the hyperparameters , and d \\nis the damping factor.  \\nChromosomes with the highest scores are \\nconsidered to be the best c urrent solution. The \\n',\n",
       " '8': 'selection of chromosomes with higher fitness \\nallows the genetic algorithm to select the \\nhighest -quality  event patterns.  \\nFor the selection of parents, the fitness of \\neach event pattern in the population was first \\nevaluated according to Equation ( 3), and a \\nportion of individuals were selected as parents \\nto participate in the next evolution according to \\nthe fitness value using the roulette algorithm \\n[26], where event patterns with higher fitness \\nhave a higher probability of being selected,  \\nwhile event patterns with lower fitness still \\nhave a certain probability of being selected, \\nwhich is conducive to maintaining the diversity \\nof the population . \\nSince the event patterns contain fewer and \\nless diverse argument roles, introducing \\nvariation op erations at this time may introduce \\nunnecessary randomness , so using only \\ncrossover operations here can maintain the \\nstability of the population and allow faster \\nconvergence to the optimal solution. The \\ncrossover operation combines the \\nchromosomes in the p ool randomly two by two \\ninto event pattern pairs and swaps its own k \\nargument roles , k being a random number no \\ngreater than the minimum number of argument \\nroles contained in the two event patterns . \\nSubsequently, the progeny individuals \\nobtained by crossov er are merged with the \\nparent individuals to form a new generation of \\npopulation. The new generation population is \\ninvolved in the selection and crossover \\noperation as the parent of the next iteration. In \\nthis paper, we set the maximum number of \\niterations  I, I am the hyperparameter , and the \\nalgorithm stops running when this number is \\nreached, and finally , the event patterns in the \\npopulation are ranked according to the fitness \\nvalue, and the individual with the highest \\nfitness value is selected to represen t the most \\naccurate and comprehensive event pattern with \\nthe highest quality as a hint for news summary \\ngeneration.  3.3 News Summary Generation  \\nWe use 3.2 to obtain highly adaptable event \\npatterns and input the highly adaptable event \\npatterns into the LLM  to generate more \\naccurate and comprehensive news summaries . \\nThe black -box characteristics  of the LLM  also \\ncause  the generated results to often lack \\ninterpretability. The event model can provide \\nexplanations for the news summaries  generated \\nby the LLM , thus  increasing their \\ninterpretability and making the generated \\nresults easier to understand and interpret.  \\nIn the second step, we extract a large number \\nof event patterns from the event representations \\ncontained in each news clip and evolve the \\npopulation con tinuously, and after a certain \\nnumber of evolutions sort the event pattern \\nwith the highest fitness s as the high-quality  \\nevent pattern. n news clips are obtained with a \\ntotal of N high-quality  event patterns  S = \\n{s1 ,s2 ,...,sN }. The set of high -quality event \\npatterns is then input into the large language \\nmodel to obtain news summaries . \\nThe quality of the generated news \\nsummaries is assessed by a series of metrics \\nsuch as ROUGE, BLEU , and Overlap scores.  \\nBy combining high -quality event patterns \\nwith the generative power of large language \\nmodels, we are able to obtain more accurate \\nand comprehensible news summaries , resulting \\nin better results in terms of information \\nconveyance and understanding. This approach \\nnot only improves the quality of news \\nsummaries but also enhances our \\nunderstanding of the news summary generation \\nprocess.  \\n4. Experiment  \\n4.1 Experimental Preparation  \\nDataset : \\nWe experimented with the PENS [27] news \\nheadline generation dataset , which contains \\n113,762 news articles in 15 topic typ es, each \\ncontaining id, headline, content and a category \\n',\n",
       " '9': 'tag, of which we only used the news headline \\nand content . The grain storage pest event \\ncorpus constructed by Xiao Le [ 28] et al. is also \\nused as a niche domain dataset for comparison \\nto explore the reliability of the summaries \\ngenerated by the news summary generator \\nproposed in this paper on a niche specialized \\ndomain, which is a grain storage pest event \\ncorpus including event types and event texts \\nobtained from grain storage pest texts after \\nevent ex traction.  \\nLLM : \\nWe use ChatGLM [29] as the backbone large \\nlanguage model of our news summarization \\nframework.  \\nThe effectiveness of the method proposed in \\nthis paper was verified by experimenting with \\ntwo different tasks:  \\n(1) The event pattern getter acquire s and \\nevolves the event pattern quality enhancement \\neffect afterward . \\n(2) The experimental results of the \\nframework on the niche domain dataset are \\ncompared with the PENS news headline \\ngeneration  dataset to demonstrate the \\ngeneralization ability of the obt ained news \\nsummaries.  \\n4.2 Results of News summary generator  \\nIn this section, the quality of news summaries \\ngenerated by NSG is evaluated, and the \\nbaseline selection of the TFIDF -based algorithm and TextRank -based algorithm for \\nnews summary generation. As w ell as a Gpt2 -\\nbased news summary generation method [ 30], \\nthe main idea is to generate multiple summaries \\nand select the most appropriate news summary \\nfrom them, while we use LLM to generate \\nsummaries directly on the text.  \\nExperimental evaluation metrics ar e used \\nROUGE, BLEU and overlap score, ROUGE is \\na text summarization evaluation metric based \\non n -gram and recall, which measures the \\nquality of summarization by examining aspects \\nsuch as word overlap, sentence -level similarity , \\nand sequence -level similarit y in summarization , \\nROUGE [31] metric system includes several \\nevaluation metrics, such as  ROUGE -1, \\nROUGE -2, and ROUGE -L, etc., which denote \\nmeasuring one -word matching, measuring \\nbinary word matching, and recording the \\nlongest common subsequence, respectiv ely, \\nand are widely used in the evaluation and \\ncomparison of text summarization tasks. BLEU \\n[32] is a precision -based similarity measure, \\nand the common metrics are BLEU -1, BLEU -\\n2, BLEU -3, and BLEU -4, Overlap  detects the \\nfrequency of occurrence of repeated  phrases \\nbetween the generated news headlines and the \\nreference headlines , and can measure to what \\nextent the model is copying phrases directly \\nfrom the text as summaries . \\n  ROUGE    BLEU     \\nModel  R-1 R-2 R-L B-1 B-2 B-3 B-4 Overlap  \\nTFIDF  0.265 0.087 0.183 0.241 0.132 0.129 0.048 59 \\nTextRank  0.354 0.143 0.282 0.312 0.188 0.174 0.082 52 \\nGpt2  0.398 0.182 0.357 0.39 0.261 0.208 0.117 42 \\nGLM  0.489 0.197  0.382  0.414  0.298  0.236  0.142  45 \\nGLM+NSG  0.568  0.224  0.403  0.433  0.315  0.26 0.174  43 \\nTable 1: Compari son of experimental results of GLM+NSG with baseline methods. We chose \\nROUGE , BLEU and Overlap  as evaluation metrics, and the values of Overlap are percentage s \\n \\nWe compare the news summary generation \\nmethod of LLM plus NSG proposed in this paper with the above baseline most, the effect \\nof LLM plus NSG for news summary \\n',\n",
       " '10': 'generation is 0.431 and 0.417 higher than \\nTFIDF and TextRank, respectively, which \\nindicates that the method of TFIDF and \\nPageRank weighting the adaptation of \\nargumentative roles and then gene rating news \\nsummaries is better than directly using TFIDF \\nand PageRank is better.  \\nAlso compared with Gpt2 and direct \\nsummary generation with a large model, the \\neffects were 0.164 and 0.079 higher, \\nrespectively, demonstrating the effectiveness of \\nextracting  event patterns before population \\nevolution.  4.3 Results in the Professional Field \\nThe experimental effect on the grain storage \\ndomain dataset , the grain storage pest dataset is \\nconstructed for the purpose of constructing a \\ngrain storage pest matter mappin g, and the \\nevent text paragraphs are short, and the data are \\nin the form of grain storage pest related events \\nand the corresponding matter , which is the \\nabstract generalization of the corresponding \\nevent, which we use as the event title for the \\nexperiment . \\n  ROUGE    BLEU     \\nModel  R-1 R-2 R-L B-1 B-2 B-3 B-4 Overlap  \\nTFIDF  0.195 0.094 0.122 0.062  0.105 0.116 0.052 63 \\nTextRank  0.221 0.113 0.168 0.274 0.131 0.134 0.063 54 \\nGpt2  0.256  0.127  0.219 0.241 0.163 0.152 0.072 38 \\nGLM  0.314 0.133 0.273 0.284 0.179 0.163 0.088 40 \\nGLM+ NSG  0.378 0.172 0.305 0.312  0.202  0.193 0.112  42 \\nTable 2: GLM+ NSG  results on stored grain pest dataset compared to baseline experiments  \\n \\nThe experimental results on the corpus of \\nstored grain pests we can see that the effect of \\nthe L LM plus NSG  approach decreases on the \\nPENS dataset, but the overall effect is still \\nbetter than the baseline, which proves the \\ngeneralization of the news summary extraction \\nparadigm proposed in this paper . On the \\nOverlap  metric, since the methods based on \\nTFIDF and TextRank are extractive methods, \\nthey are more dependent on the original data \\nwhen the amount of data is small, so the \\nobtained event summaries have a higher \\nOverlap  rate with the original text, while the \\nmethods based on GPT2 and LLM and the \\nproposed method in this paper are generative, \\nso the Overlap  rate is reduced.  \\nExperimental results show that the proposed \\nmethod in this paper still achieves good results \\non the grain storage pest dataset, proving to be \\nequally effective for summary generatio n tasks \\nin niche domains . 4. Conclusion  \\nIn this paper, we propose a new paradigm for \\nnews summary generation and design a News \\nSummary  Generator  (NSG) . Experimental \\nverif y that the proposed approach in this paper \\nis able to generate accurate and reliable news  \\nsummaries on news summary generation tasks \\nwith certain generalizations  and application to \\nrelated tasks. At the same time, by introducing \\nthe event model into the news summary \\ngeneration process of the LLM , the problem of \\nits lack of interpretability can  be solved to a \\ncertain extent, enabling us to understand and \\nexplain the internal operation of the model  \\nbetter , as well as the basis and logic of \\ngenerating news summaries. This helps to \\nimprove the credibility and reliability of the \\nLLM  and provide more  reliable and \\ninterpretable results for the news summary \\ngeneration task. \\n',\n",
       " '11': ' \\n \\n \\n \\nReferen ce: \\n[1] Devlin J, Chang M W, Lee K,  et al.BERT: \\nPre-training of Deep Bidirectional \\nTransformers for Language Understanding[J].  \\n2018 , pages  4171 –4186  \\n[2] Lewis  M, Liu  Y, Goyal N, et al.  BART: \\nDenoising sequence -to-sequence pretraining \\nfor natural language generation, translation,  \\nand comprehension [C]// In Proc. of ACL, \\npages 7871 –7880  \\n[3] Brown T B, Mann B, Ryder N,  et al.  \\nLanguage Models are Few -Shot Learners[J].  \\n2020 , NeurIPS  2020,  pages 6 -12 \\n[4] OpenAI. ChatGPT: Optimizing language \\nmodels for dialogue, 2022.  \\n[5] OpenAI. GPT -4 technical report. arXiv \\npreprint arXiv:2303.08774, 2023.  \\n[6] Scao T, Fan A, Akiki C, et al. BLOOM: A \\n176B -Parameter Open -Access Multilingual \\nLangua ge Model [J]. 2023 , ArXiv preprint, \\nabs/2211.05100.  \\n[7] Vaswani A, Ahazeer N, Parmar N, et al. \\nAttention is All You Need [C]// In Advances in \\nNeural Information Processing Systems 30: \\nAnnual Conference on Neural  Information \\nProcessing Systems 2017 , pages  5998-6008  \\n[8] Wei X, Cui X, Cheng N, et al. Zero -Shot \\nInformation Extraction via Chatting with \\nChatGPT  [J]. arXiv:2302.10205 , 2023  \\n[9] Kıcıman  E, Ness R, Sharma A, et al. Causal \\nReasoning and Large Language Mod els: \\nOpening a New Frontier for Causality [J]. \\narXiv:2305.00050 ,2023  \\n[10] Wang M, Tanaka H, Zhong Y .1 \\nGenerating Summaries of Multiple Technical \\nArticles[J].  2023  \\n[11] Wang W, Shi C, Yu X, et al. An extracti ve \\ntopic brief representation generation method to \\nevent [J]. Journal of Shandong University  \\n(Natural Science) , 2021,56(05):66 -75+84.  [12] Han Y, Xu X, Li B , et al.  Web News Multi -\\ndocument Summarization Based on Event \\nExtraction [J]. Journal of Chinese Infor mation \\nProcessing, 2012,26(01):58 -66. \\n[13] Atapattu T, Falkner K. A Framework for \\nTopic Generation and Labeling from MOOC \\nDiscussions [J]. Proceedings of the Third ACM \\nConference on Learning , 2016, pages 201 -204  \\n[14] Hong Y, Zhang J, Ma B,  et al.Using Cross -\\nEntity Inference to Improve Event \\nExtraction[C]//  Meeting of the Association for \\nComputational Linguistics:  Human Language \\nTechnologies.  Association for Computational \\nLinguistics, 2011.  \\n[15] LI Q,  LI J,  SHENG J,  et al. A Compact \\nSurvey on Event Extraction: Approaches and \\nApplications[J]. arXiv:2107.02126 , 2021  \\n[16] CHEN Y,  XU L,  LIU K,  et al. Event \\nExtraction via Dynamic Multi -Pooling \\nConvolutional Neural Networks [C]// \\nInternational Joint Conference on Natural \\nLanguage Processing,  2015 , pages 167-176 \\n[17] SUBBURATHINAM A,LU D, JI H,  et al. \\nCross -lingual structure transfer for relation and \\nevent[C]// International Joint Conference on \\nNatural Language Processing , 2019 , pages \\n313-325 \\n[18] ZHANG J,  QIN Y ,  ZHANG Y ,  et al.  \\nExtracting entities and events as a single task \\nusing a transition -based neural model[C]//  \\nInternational Joint Conference on Artif icial \\nIntelligence,2019 , pages 5422 -5428  \\n[19] LI D,HUANG L,JI H,  et al. Biomedical \\nevent extraction based on knowledge -driven \\ntree-lstm[C]//  Human  Language  Technologies , \\n2019 , pages 1421 -1430  \\n[20] Chambers N, Jurafsky D. Template -based \\ninformation extraction without  the templates. \\n[C]//  In Proceedings of the 49th Annual  \\nMeeting of the Association for Computational  \\nLinguistics: Human Language Technologies, \\n2011, pages  976–986 \\n',\n",
       " '12': '[21] Cao B, Lin H, Han X, e t al. The Life Cycle \\nof Knowledge in Big Language Models: A \\nSurvey [J]. arXiv:2307.01189 , 2023  \\n[22] Goldberg D. E., Holland J. H. Genetic \\nalgorithms and machine learning. machine \\nlearning [J], Machine Learning , 1988, 3(2), \\npages 95-99 \\n[23] Gehring J, Auli M, Grangier D,  et al. \\nConvolutional sequence to sequence \\nlearning [J]. Proceedings of the 34th \\nInternational Conference on Machine Learning, \\n2017, pages 1243 -1252  \\n[24] Li J, Tang T, Zhao W, et al. Pretrained \\nLanguage Models for Text Generation: A \\nSurvey [J], arXiv:2201.05273 , 2022  \\n[25] R. Mihalcea and P. Tarau, TextRank: \\nBringing order into text, in Proc. 2004 \\nConference  on Empirical Methods in Natural \\nLanguage Processing, 2004,  pages  404–411 \\n[26] Lipowski A, Lipowska D.  Roulette -wheel \\nselection via stochastic acceptance[J]. Physica \\nA Statistical Mechanics & Its Applications, \\n2012, 391(6):2193 -2196.   \\n[27] Ao X, Wang X, Luo L, et al. PENS: A \\nDataset and Generic Framework for \\nPersonalized News Headline Generation [C]// \\nThe Annual Meeting of  the Association for \\nComputational Linguistics (ACL) , 2021  \\n[28] Xiao L , Chen X, Shan X.  Construction of \\nStored Grain Pest Event Evolutionary Graph  \\n[J]. Journal of the Chinese Cereals and Oils \\nAssociation , 2022  \\n[29] Du Z, Qian Y, Liu X, et al. GLM: General  \\nLanguage Model Pretraining with \\nAutoregressive Blank Infilling [J]. \\narXiv:2103.10360 , 2023  \\n[30] Mishra P, Diwan C, Srinivasa s , et al. \\nAutomatic Title Generation for Text with Pre -\\ntrained Transformer Languag e Model [C]// \\n2021 IEEE 15th International Conference on \\nSemantic Computing (ICSC), 2021, pages 17 -\\n24 \\n[31] Lin C. ROUGE: A Package for Automatic \\nEvaluation of Summa ries[C]// Post-Conference Workshop of ACL 2004  \\n[32] Papineni K , Roukos S , Ward T , et al. \\nBLEU: a Method for Automatic Evaluation of \\nMachine Translation [C]//  Proceedings of the \\n40th Annual Meeting on Association for \\nComputational Linguistics , 2002 , pages 311 -\\n318 \\n \\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict={}\n",
    "for document in docs:\n",
    "    data_dict[document.metadata['source']]=document.page_content\n",
    "data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_summaries={}\n",
    "for page,content in data_dict.items():\n",
    "        \n",
    "        response=client.chat.completions.create(model=\"gpt-3.5-turbo\",messages=[\n",
    "        {\n",
    "        \"role\":\"system\", \"content\":sys_prompt\n",
    "        },\n",
    "        {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"\"\"You are provided with the PDF text:\n",
    "     content: {content} \n",
    "     Your task is to summarize the provided document in 300 words\"\"\"\n",
    "    }\n",
    "    ]\n",
    "    )\n",
    "        summary=response.choices[0].message.content\n",
    "        page_summaries[page]=summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'The document discusses the enhancement of news summary generation through the utilization of Large Language Models (LLM) with evolutionary fine-tuning. The traditional methods of news summary generation face challenges due to model limitations, training data constraints, and text noise. The proposed approach involves using LLM to extract structured event patterns from news paragraphs, evolving them with a genetic algorithm, and selecting the most suitable event pattern to generate news summaries. A News Summary Generator (NSG) is designed for this purpose to select and evolve event pattern populations, resulting in accurate and reliable news summaries with some level of generalization ability.\\n\\nPre-trained language models, particularly those based on the Transformer architecture, have seen significant advancements, enabling them to process natural language and learn statistical patterns from large-scale textual data. LLMs have shown improvements in various natural language processing tasks, surpassing previous state-of-the-art models in areas like information extraction and causal inference. The development of LLM performance in specific tasks, such as news summary generation, has garnered considerable research attention.\\n\\nNews summary generation is crucial for conveying essential event topics concisely from text paragraphs to facilitate intelligent content communication. It plays a vital role in domains like information processing, intelligence analysis, research, and decision-making. Automatic news summary generation can provide comprehensive information to enhance understanding and response to complex real-world events. The generated news summaries aim to capture important details to aid in better comprehension and response to various scenarios, showcasing the significance of this task in information dissemination and analysis.',\n",
       " '2': \"The document discusses challenges faced by existing news headline generation methods due to model limitations and training data constraints. To address these challenges, a new paradigm using Language Model (LLM) to generate news summaries is proposed. The approach involves extracting event patterns from news paragraphs, evolving these patterns using a genetic algorithm, and evaluating them based on fitness. The best patterns are then used to enhance LLM for generating accurate and reliable news summaries. The process includes unsupervised event pattern pool generation, evolutionary population evolution, and fine-tuning LLM to produce summaries. The main contributions include improving LLM's capability by fine-tuning it with evolutionary knowledge and addressing issues of inaccuracies in generated summaries. A news summary generator is designed to extract event patterns from open-world knowledge using LLM, iteratively updating these patterns, and selecting the most adaptive ones to enhance LLM for creating comprehensive and accurate summaries. Experimental results show that the generator can effectively summarize news texts with concise and accurate natural language descriptions, aiding in better understanding and conveying core event content, particularly beneficial for intelligence analysis.\",\n",
       " '3': 'The document discusses research on news summary generation, event extraction, knowledge acquisition using LLM, and genetic algorithms. Event extraction involves trigger identification (identifying keywords indicating events) and event argument identification/classification (identifying event details like participants, time, place). Challenges include semantic complexity and context dependency, requiring techniques like lexical analysis and syntactic parsing. It supports applications like information extraction and knowledge graph construction. Traditional machine learning event extraction methods are pipeline-based or union-based. Event types and meta-roles form an event model describing events comprehensively. The event schema includes event types and meta-roles representing different aspects of events like participants and actions. Argumentative meta-roles aim to universally represent concepts like Subject, Object, Time, Place. Event patterns offer a structured way to represent and analyze events, capturing core information and key elements. Natural language generation relies on event patterns to generate corresponding text. Overall, event extraction is crucial for extracting and organizing event information from text data for various practical applications, using techniques like trigger word identification and argument classification.',\n",
       " '4': \"The document discusses the use of natural language descriptions for event patterns to generate accurate representations for news headlines and summaries. It introduces Genetic Algorithm (GA) as an optimization technique based on biological evolution theory to search and optimize solutions through genetic operations like selection, crossover, and mutation. The GA process involves creating a population of candidate solutions, improving them iteratively, and evolving individuals based on selection and crossover. The algorithm uses an adaptation function to assess individuals' accuracy and select parent solutions for the next generation. After multiple iterations, the individuals with the highest fitness value are filtered out as event patterns that can effectively summarize key information from texts.\\n\\nSummarizing text is crucial for intelligence analysis, as it helps in organizing large amounts of data, extracting key information, and enhancing decision-making efficiency. By unifying and summarizing diverse event representations into concise natural language descriptions, redundant information is eliminated, and the intelligence analysis process becomes more effective. Traditional news summaries are generated by human-designed rules and templates or text clustering methods. Generating unified summaries from a large number of similar events is essential for niche domains' intelligence analysis needs. Overall, text summarization facilitates intelligence work by providing decision support and guidance through efficient extraction of important information from large datasets.\",\n",
       " '5': 'The document discusses the evolution of news summary generation techniques, moving from manually edited summaries to modern approaches using deep learning, specifically neural network-based models. These models have overcome challenges such as data sparsity but can suffer from overfitting due to a lack of large-scale training data. The emergence of Large Language Models (LLM) has shown promise in natural language processing tasks, although their black-box nature limits interpretability. Event patterns are proposed as a way to enhance understanding of how LLM generates high-quality news summaries by leveraging event-related information to capture key features and contextual details.\\n\\nLLM offers advantages in knowledge acquisition by understanding and extracting information from large text datasets but may face limitations in news summary quality, accuracy, and coverage. To address these challenges, a new paradigm is introduced for LLM-based news summary generation. The proposed method involves extracting event patterns from news fragments, evolving them using a genetic algorithm, and inputting the highest-scoring patterns into the LLM to generate concise and accurate news summaries. The process aims to distill key information from texts efficiently.\\n\\nIn conclusion, the document highlights the importance of balancing accuracy and comprehensiveness in news summary generation while leveraging LLM capabilities for knowledge extraction. By evolving event patterns and utilizing advanced models, the proposed method aims to improve the quality and relevance of news summaries extracted from large volumes of text data.',\n",
       " '6': \"The document discusses the development of a News Summary Generator (NSG) aimed at enhancing news summary generation using Large Language Models (LLM). The framework of NSG consists of three main components: Candidate event pattern pool generation, Event pattern population evolution, and News summary generation. Event patterns play a crucial role in summarizing news by abstracting and generalizing event representations to extract key information efficiently from a large volume of data. Real-world text data presents challenges like noise and diversity, requiring models with robustness and generalization capabilities to accurately extract event patterns. Traditional neural network-based methods for event pattern extraction rely on substantial training data, which may be limited in some domains, affecting the model's ability to learn effectively. \\n\\nThe document highlights the importance of contextual understanding and inference of implicit event information to capture event patterns accurately. LLM is leveraged for its language generation and context learning capabilities by unsupervised and automatic generation of candidate event patterns from original event data. The model's ability to understand contextual dependencies and infer implicit information is crucial for effective summarization. By utilizing LLM and advanced techniques, the NSG aims to improve news summary generation by efficiently extracting relevant information and enhancing understanding and summarization of events.\",\n",
       " '7': 'The document discusses utilizing Language Model (LLM) for contextual fine-tuning to generate event patterns for news summarization. Contextual fine-tuning aims to enhance model performance by leveraging pre-trained language knowledge and refining it for specific tasks, allowing for better task-specific output generation. The process involves creating event representations paired with event patterns to guide LLM for generating news summaries. The concept of Knowledge Evolution integrates a genetic algorithm to produce news summaries by combining good genomes and selecting event patterns with higher adaptability. Fitness functions assess the salience and reliability of event patterns based on meta-role frequencies and importance, influencing the accuracy and comprehensiveness of generated summaries. Algorithmic scores like TF-IDF and TextRank are used to evaluate event patterns within populations. The fitness function Q combines TF-IDF and TextRank scores to determine the weighting of argumentative meta-role in event patterns. Hyperparameters and damping factor influence the function, with chromosomes having the highest scores considered as optimal solutions for news summarization. This methodic approach aims to enhance the summarization process by refining event patterns and selecting the most effective ones for generating comprehensive news summaries.',\n",
       " '8': 'The document discusses a genetic algorithm approach for generating news summaries. Initially, chromosomes with higher fitness are selected to improve the quality of event patterns. Using a roulette algorithm, parents are chosen based on fitness to ensure diversity in the population. The crossover operation combines chromosomes to create new individuals, maintaining population stability. The process iterates until a specified maximum number of iterations is reached. Fitness-ranked event patterns are used to represent accurate summaries. These summaries are generated by inputting adaptable event patterns into a large language model (LLM). The event model enhances interpretability of LLM-generated results and improves understanding of news summaries. Through continuous evolution and selection of high-quality event patterns, more precise news summaries are obtained. Evaluation metrics such as ROUGE, BLEU, and Overlap scores assess the quality of generated summaries. The combination of high-quality event patterns and LLMs results in enhanced information conveyance and comprehension. Experimenting with the PENS news dataset, consisting of various news articles, validates the proposed approach for generating accurate and comprehensive news summaries across different topics.',\n",
       " '9': 'The document evaluates a news summary generator (NSG) using a large language model (LLM) in the niche domain of grain storage pest events. The study compares the effectiveness of NSG with other baseline methods like TFIDF, TextRank, and Gpt2 in generating news summaries. Evaluation metrics used include ROUGE and BLEU scores, as well as Overlap percentages to assess the quality of summarization by measuring word overlap, sentence-level similarity, and sequence-level similarities.\\n\\nThe results show that the combination of LLM and NSG (GLM+NSG) outperforms the baseline methods in terms of ROUGE and BLEU scores. GLM+NSG achieves higher scores in one-word matching (ROUGE-1) and binary word matching (ROUGE-2), indicating better summarization quality. The comparison also includes Overlap percentages, with GLM+NSG demonstrating a higher score, suggesting improved summarization output with reduced copying of phrases from the source text.\\n\\nThe study affirms the reliability and generalization ability of the NSG framework in summarizing news within the niche domain of grain storage pest events. By utilizing a backbone large language model like ChatGLM, the NSG framework enhances the quality and accuracy of news summaries, making it a viable tool for generating concise and informative news headlines. The results highlight the effectiveness of LLM in directly generating summaries and showcase the potential of NSG in improving news summarization tasks.',\n",
       " '10': 'The study compares different methods for news summary generation, finding that combining TFIDF and TextRank for adaptation of argumentative roles and news summary generation leads to better results than using them directly. When compared with Gpt2 and direct summary generation using large models, the method shows higher effectiveness in extracting event patterns before summary generation. Results on a grain storage pest dataset show that the GLM+ NSG approach performs better than baseline methods. The GLM+ NSG approach maintains effectiveness on the PENS dataset, demonstrating the generalization of the news summary extraction paradigm proposed. While TFIDF and TextRank methods have higher overlap rates with the original text due to their extractive nature, GPT2 and generative methods reduce overlap rates. The proposed method in the paper achieves good results on the grain storage pest dataset and proves effective for summary generation in niche domains. The study introduces a new paradigm for news summary generation using the News Summary Generator (NSG). Experimental results support the accuracy and reliability of the proposed approach in generating news summaries with generalizations and applications to relevant tasks. By incorporating an event model into the summary generation process, the lack of interpretability in Large Language Models (LLM) is addressed, leading to improved understanding, credibility, and reliability of the model for generating news summaries.',\n",
       " '11': 'The document provides a comprehensive list of references related to language understanding, generation, and event extraction through the use of various models and techniques. These references cover a range of cutting-edge research in the field of natural language processing (NLP).\\n\\nSome key references include the work on BERT, which is a deep bidirectional transformer model for language understanding, and BART, a denoising sequence-to-sequence pretraining model for natural language generation, translation, and comprehension. ChatGPT and GPT-4 are language models optimized for dialogue and technical reports.\\n\\nAdditionally, the document includes references on large language models, zero-shot information extraction, causal reasoning, and event extraction methods like dynamic multi-pooling convolutional neural networks and knowledge-driven tree-lstm for biomedical event extraction.\\n\\nThere are also references on topic generation and labeling from MOOC discussions, cross-lingual structure transfer for relation and event, and extracting entities and events as a single task using transition-based neural models.\\n\\nOverall, the document showcases the diverse range of research efforts and advancements in NLP, highlighting the significance of various models and techniques in advancing the field of language understanding, generation, and event extraction.',\n",
       " '12': 'The document provides a list of references on various topics related to machine learning, natural language processing, and computational linguistics. It includes surveys on knowledge life cycles in big language models, genetic algorithms, convolutional sequence to sequence learning, pretrained language models for text generation, and personalized news headline generation. Additionally, it covers specific methodologies such as TextRank for organizing text, roulette-wheel selection for stochastic acceptance, and title generation for text using pre-trained transformer language models.\\n\\nThe references also delve into specific models and frameworks such as PENS, a dataset and framework for personalized news headline generation, GLM for general language model pretraining with autoregressive blank infilling, and stored grain pest event evolutionary graph construction. Furthermore, the document discusses tools like ROUGE for automatic summary evaluation and BLEU for automatic evaluation of machine translation.\\n\\nOverall, the document highlights key research papers, conference proceedings, and methods in the fields of machine learning, natural language processing, and computational linguistics. By referencing these works, researchers and practitioners can access a wide range of studies, methodologies, and tools to enhance their understanding and development in these areas.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=client.chat.completions.create(model=\"gpt-3.5-turbo\",messages=[\n",
    "        {\n",
    "        \"role\":\"system\", \"content\":sys_prompt\n",
    "        },\n",
    "        {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"\"\"You are provided with the dictionary with key as the page numbers and values are the summaries of the respective page:\n",
    "     content: {page_summaries} \n",
    "     Your task is to combine all the summaries and provide a collated final summary of the document in 300 words\"\"\"\n",
    "    }\n",
    "    ]\n",
    "    )\n",
    "Final_summary=response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document explores the utilization of Large Language Models (LLMs) with evolutionary fine-tuning to enhance news summary generation. Traditional methods face challenges such as model limitations and text noise. LLMs, particularly based on Transformer architecture, have advanced in natural language processing tasks, excelling in information extraction. News summary generation is crucial for conveying essential event topics concisely, aiding in intelligence analysis and decision-making. The proposed approach involves extracting event patterns from news paragraphs using LLMs and genetic algorithms to generate accurate and reliable news summaries.\\n\\nEvent extraction involves trigger identification and event argument identification/classification, supporting applications like information extraction and knowledge graph construction. Genetic algorithms are introduced for optimizing event patterns to create news headlines and summarizations efficiently. Evaluations include ROUGE and BLEU scores for summarization quality assessment. The document emphasizes the importance of balancing accuracy and comprehensiveness in news summary generation while leveraging LLM capabilities. The News Summary Generator framework extracts and evolves event patterns to improve summarization accuracy.\\n\\nThe study compares different methods for news summary generation, demonstrating that combining TFIDF and TextRank yields better results. The GLM+NSG approach outperforms baseline methods, showcasing effectiveness in both niche domains and generalization tasks. References highlight cutting-edge research in NLP, covering models like BERT, BART, ChatGPT, and GPT-4. Overall, the document provides a comprehensive overview of advancements in language understanding, generation, and event extraction, showcasing the significance of various models and techniques in advancing natural language processing.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User providing Data Manually which is then summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"\"\"Enhancing LLM with Evolutionary Fine -Tuning for News \\nSummary Generation  \\nLe Xiao, Xiaolin Chen  \\nCollege of Information Science and Engineering, Henan University of Technology, \\nZhengzhou, China  \\nxiaole@haut.edu.cn chenxiaolin @stu.haut.edu.cn  \\nAbstract  \\nNew s summary generation is an \\nimportant task in the field of intelligence \\nanalysis, which can provide accurate and \\ncomprehensive information to help \\npeople better understand and respond to \\ncomplex real -world events. However, \\ntraditional news summary generatio n \\nmethods face some challenges, which are \\nlimited by the model itself and the \\namount of training data, as well as the \\ninfluence of text noise, making it difficult \\nto generate reliable information \\naccurately. In this paper, we propose a \\nnew paradigm for new s summary \\ngeneration using LLM  with powerful \\nnatural language understanding and \\ngenerative capabilities. We use LLM  to \\nextract multiple structured event patterns \\nfrom the events contained in news \\nparagraphs, evolve the event pattern \\npopulation with genetic  algorithm, and \\nselect the most adaptive event pattern to \\ninput into the LLM  to generate news \\nsummaries. A News Summary Generator  \\n(NSG ) is designed to select and evolve \\nthe event pattern populations and \\ngenerate news summaries. The \\nexperimental results sho w that the news \\nsummary generator is able to generate \\naccurate and reliable news summaries \\nwith some generalization ability.  \\n 1. Introduction  \\nIn recent years, pre -trained language models \\nhave undergone rapid development [1, 2, 3, 4, \\n5, 6], especially models b ased on the \\nTransformer [7] architecture, which has the \\nability to process natural language. These \\nmodels are able to automatically learn \\nstatistical patterns and patterns in language by \\ntraining on large -scale textual data , which \\nmakes pre -trained languag e models widely \\nadaptable and can be applied to a variety of \\ndomains and tasks.  \\nLarge language m odels  (LLM)  have \\nimproved the experimental results of many \\nnatural language processing tasks, exceeding \\nthe previous state -of-the-art of deep learning \\nmodels in  tasks such as information extraction \\n[8] and causal inference [9], and therefore, how \\nto enhance the performance of LLMs in \\nspecific tasks has attracted extensive research.  \\nNews summary generation is a type of \\ndocument summary generation [10], which \\naims to generate concise and important event \\ntopics in a paragraph of text to better \\ncommunicate intelligent  content. It plays a key \\nrole in areas such as information processing, \\nintelligence analysis, research , and decision -\\nmaking . Automatic news summary gener ation \\ncan provide accurate and comprehensive \\ninformation to help people better understand \\nand respond to complex real -world events.  \\nTraditional news summary generation is \\nmainly used to generate headlines , a task that \\nrequires the model to be able to under stand the data.\n",
    "\"\"\"\n",
    "processed_data = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100,length_function=len,separators=['\\n'])\n",
    "splits = text_splitter.split_text(data)\n",
    "for i,text in enumerate(splits):\n",
    "    processed_data.extend(\n",
    "            [\n",
    "                Document(\n",
    "                    page_content= text,\n",
    "                    metadata={\"source\": i}\n",
    "                ) \n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 0}, page_content='Enhancing LLM with Evolutionary Fine -Tuning for News \\nSummary Generation  \\nLe Xiao, Xiaolin Chen  \\nCollege of Information Science and Engineering, Henan University of Technology, \\nZhengzhou, China  \\nxiaole@haut.edu.cn chenxiaolin @stu.haut.edu.cn  \\nAbstract  \\nNew s summary generation is an \\nimportant task in the field of intelligence \\nanalysis, which can provide accurate and \\ncomprehensive information to help \\npeople better understand and respond to \\ncomplex real -world events. However, \\ntraditional news summary generatio n \\nmethods face some challenges, which are \\nlimited by the model itself and the \\namount of training data, as well as the \\ninfluence of text noise, making it difficult \\nto generate reliable information \\naccurately. In this paper, we propose a \\nnew paradigm for new s summary \\ngeneration using LLM  with powerful \\nnatural language understanding and \\ngenerative capabilities. We use LLM  to \\nextract multiple structured event patterns \\nfrom the events contained in news'),\n",
       " Document(metadata={'source': 1}, page_content='extract multiple structured event patterns \\nfrom the events contained in news \\nparagraphs, evolve the event pattern \\npopulation with genetic  algorithm, and \\nselect the most adaptive event pattern to \\ninput into the LLM  to generate news \\nsummaries. A News Summary Generator  \\n(NSG ) is designed to select and evolve \\nthe event pattern populations and \\ngenerate news summaries. The \\nexperimental results sho w that the news \\nsummary generator is able to generate \\naccurate and reliable news summaries \\nwith some generalization ability.  \\n 1. Introduction  \\nIn recent years, pre -trained language models \\nhave undergone rapid development [1, 2, 3, 4, \\n5, 6], especially models b ased on the \\nTransformer [7] architecture, which has the \\nability to process natural language. These \\nmodels are able to automatically learn \\nstatistical patterns and patterns in language by \\ntraining on large -scale textual data , which \\nmakes pre -trained languag e models widely'),\n",
       " Document(metadata={'source': 2}, page_content='training on large -scale textual data , which \\nmakes pre -trained languag e models widely \\nadaptable and can be applied to a variety of \\ndomains and tasks.  \\nLarge language m odels  (LLM)  have \\nimproved the experimental results of many \\nnatural language processing tasks, exceeding \\nthe previous state -of-the-art of deep learning \\nmodels in  tasks such as information extraction \\n[8] and causal inference [9], and therefore, how \\nto enhance the performance of LLMs in \\nspecific tasks has attracted extensive research.  \\nNews summary generation is a type of \\ndocument summary generation [10], which \\naims to generate concise and important event \\ntopics in a paragraph of text to better \\ncommunicate intelligent  content. It plays a key \\nrole in areas such as information processing, \\nintelligence analysis, research , and decision -\\nmaking . Automatic news summary gener ation \\ncan provide accurate and comprehensive \\ninformation to help people better understand'),\n",
       " Document(metadata={'source': 3}, page_content='can provide accurate and comprehensive \\ninformation to help people better understand \\nand respond to complex real -world events.  \\nTraditional news summary generation is \\nmainly used to generate headlines , a task that \\nrequires the model to be able to under stand the data.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Enhancing LLM with Evolutionary Fine -Tuning for News \\nSummary Generation  \\nLe Xiao, Xiaolin Chen  \\nCollege of Information Science and Engineering, Henan University of Technology, \\nZhengzhou, China  \\nxiaole@haut.edu.cn chenxiaolin @stu.haut.edu.cn  \\nAbstract  \\nNew s summary generation is an \\nimportant task in the field of intelligence \\nanalysis, which can provide accurate and \\ncomprehensive information to help \\npeople better understand and respond to \\ncomplex real -world events. However, \\ntraditional news summary generatio n \\nmethods face some challenges, which are \\nlimited by the model itself and the \\namount of training data, as well as the \\ninfluence of text noise, making it difficult \\nto generate reliable information \\naccurately. In this paper, we propose a \\nnew paradigm for new s summary \\ngeneration using LLM  with powerful \\nnatural language understanding and \\ngenerative capabilities. We use LLM  to \\nextract multiple structured event patterns \\nfrom the events contained in news',\n",
       " 1: 'extract multiple structured event patterns \\nfrom the events contained in news \\nparagraphs, evolve the event pattern \\npopulation with genetic  algorithm, and \\nselect the most adaptive event pattern to \\ninput into the LLM  to generate news \\nsummaries. A News Summary Generator  \\n(NSG ) is designed to select and evolve \\nthe event pattern populations and \\ngenerate news summaries. The \\nexperimental results sho w that the news \\nsummary generator is able to generate \\naccurate and reliable news summaries \\nwith some generalization ability.  \\n 1. Introduction  \\nIn recent years, pre -trained language models \\nhave undergone rapid development [1, 2, 3, 4, \\n5, 6], especially models b ased on the \\nTransformer [7] architecture, which has the \\nability to process natural language. These \\nmodels are able to automatically learn \\nstatistical patterns and patterns in language by \\ntraining on large -scale textual data , which \\nmakes pre -trained languag e models widely',\n",
       " 2: 'training on large -scale textual data , which \\nmakes pre -trained languag e models widely \\nadaptable and can be applied to a variety of \\ndomains and tasks.  \\nLarge language m odels  (LLM)  have \\nimproved the experimental results of many \\nnatural language processing tasks, exceeding \\nthe previous state -of-the-art of deep learning \\nmodels in  tasks such as information extraction \\n[8] and causal inference [9], and therefore, how \\nto enhance the performance of LLMs in \\nspecific tasks has attracted extensive research.  \\nNews summary generation is a type of \\ndocument summary generation [10], which \\naims to generate concise and important event \\ntopics in a paragraph of text to better \\ncommunicate intelligent  content. It plays a key \\nrole in areas such as information processing, \\nintelligence analysis, research , and decision -\\nmaking . Automatic news summary gener ation \\ncan provide accurate and comprehensive \\ninformation to help people better understand',\n",
       " 3: 'can provide accurate and comprehensive \\ninformation to help people better understand \\nand respond to complex real -world events.  \\nTraditional news summary generation is \\nmainly used to generate headlines , a task that \\nrequires the model to be able to under stand the data.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict1={}\n",
    "for document in processed_data:\n",
    "    data_dict1[document.metadata['source']]=document.page_content\n",
    "data_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'The document discusses enhancing news summary generation using Large Language Models (LLM) with Evolutionary Fine-Tuning. News summary generation plays a crucial role in intelligence analysis by providing accurate information to help individuals better grasp and react to real-world events. Traditional methods face challenges due to model limitations, insufficient training data, and text noise impact, hindering the generation of reliable information effectively. The proposed approach leverages LLM with strong natural language understanding and generative capabilities, aiding in extracting structured event patterns from news content. By implementing Evolutionary Fine-Tuning, the model can adapt and improve its performance over time. This methodology aims to overcome the limitations of traditional approaches and enhance the accuracy and comprehensiveness of news summaries. The study is conducted by Le Xiao and Xiaolin Chen from the College of Information Science and Engineering at Henan University of Technology in Zhengzhou, China. Their research suggests a shift towards more advanced techniques like LLM and evolutionary fine-tuning to address the challenges faced in news summary generation, ultimately aiming for better information accuracy and understanding of complex real-world events.',\n",
       " 1: 'The document discusses the development of a News Summary Generator (NSG) which extracts structured event patterns from news paragraphs, utilizes genetic algorithms to evolve these patterns, and selects the most suitable pattern for input into the Large Language Model (LLM) to generate news summaries. The NSG is designed to effectively choose and refine event pattern populations to produce accurate and reliable news summaries with some level of generalization. \\n\\nThe introduction highlights the significant advancements in pre-trained language models, particularly those based on the Transformer architecture, capable of processing natural language. These models can autonomously learn statistical language patterns by training on extensive textual data, enhancing their ability to comprehend and generate coherent text.\\n\\nThe research emphasizes the importance of leveraging pre-trained language models to enhance the efficiency and accuracy of news summary generation. By extracting, evolving, and selecting event patterns using genetic algorithms, the NSG demonstrates the capability to produce informative news summaries that capture essential information from news paragraphs. The experimental results validate the effectiveness of the NSG in generating precise summaries with a degree of generalization. \\n\\nIn conclusion, the document showcases the successful integration of genetic algorithms and pre-trained language models in the development of a News Summary Generator, highlighting its ability to generate accurate, reliable, and generalized news summaries by efficiently capturing and analyzing event patterns from news content.',\n",
       " 2: 'The document discusses the benefits of training on large-scale textual data, leading to the widespread applicability of pre-trained language models (LLMs) across various domains and tasks. LLMs have shown significant improvements in natural language processing tasks, surpassing previous deep learning models in areas like information extraction and causal inference. As a result, there is a growing interest in enhancing the performance of LLMs for specific tasks through extensive research efforts.\\n\\nOne particular application highlighted in the document is news summary generation, which falls under the category of document summary generation. The goal of news summary generation is to produce concise and relevant event summaries from a text paragraph to aid in effectively conveying intelligent content. This process is crucial in fields such as information processing, intelligence analysis, research, and decision-making. Automatic news summary generation is seen as a valuable tool for offering accurate and comprehensive information to facilitate better understanding for individuals.\\n\\nOverall, the document emphasizes the significance of utilizing LLMs for various natural language processing tasks and highlights news summary generation as a relevant application that can benefit from these models. The ability to generate accurate and informative summaries from textual content has the potential to greatly assist individuals in comprehending complex information more efficiently.',\n",
       " 3: \"The document discusses the importance of providing accurate and comprehensive information to assist individuals in understanding and responding to complex real-world events. It highlights the role of traditional news summary generation in creating headlines, emphasizing the need for the model to comprehend the data in order to effectively generate these headlines. By focusing on generating headlines, the model can help users quickly grasp the key points of a story. This process requires the model to analyze and understand the content to accurately summarize it in a concise manner.\\n\\nFurthermore, the document suggests that accurate and comprehensive information is crucial for individuals to make informed decisions and take appropriate actions in response to various real-world events. By utilizing advanced algorithms and technologies, news summary generation can provide valuable insights to users, enabling them to stay informed and up-to-date on important developments. The ability to process and interpret large amounts of data efficiently is vital for generating accurate and insightful summaries that capture the essence of a story.\\n\\nIn conclusion, the document underscores the significance of accurate and comprehensive information in enhancing individuals' understanding and response to complex real-world events. It emphasizes the importance of traditional news summary generation in distilling key information into concise headlines, and how advancements in technology can further improve the quality and effectiveness of these summaries. Ultimately, providing users with reliable and digestible information is essential for empowering them to navigate and engage with the ever-changing landscape of current events.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_summaries={}\n",
    "for page,content in data_dict1.items():\n",
    "    response=client.chat.completions.create(model=\"gpt-3.5-turbo\",messages=[\n",
    "        {\n",
    "        \"role\":\"system\", \"content\":sys_prompt\n",
    "        },\n",
    "        {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"\"\"You are provided with the PDF text:\n",
    "     content: {content} \n",
    "     Your task is to summarize the provided document in 300 words\"\"\"\n",
    "    }\n",
    "    ]\n",
    "    )\n",
    "    summary=response.choices[0].message.content\n",
    "    page_summaries[page]=summary \n",
    "page_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=client.chat.completions.create(model=\"gpt-3.5-turbo\",messages=[\n",
    "        {\n",
    "        \"role\":\"system\", \"content\":sys_prompt\n",
    "        },\n",
    "        {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"\"\"You are provided with the dictionary with key as the page numbers and values are the summaries of the respective page:\n",
    "     content: {page_summaries} \n",
    "     Your task is to combine all the summaries and provide a collated final summary of the document in 300 words\"\"\"\n",
    "    }\n",
    "    ]\n",
    "    )\n",
    "Final_summary=response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document focuses on enhancing news summary generation through Large Language Models (LLM) with Evolutionary Fine-Tuning. It addresses challenges faced by traditional methods such as model limitations and text noise impact. The approach leverages LLM's natural language understanding and generative capabilities to extract structured event patterns from news content and improve performance over time with Evolutionary Fine-Tuning. The research by Le Xiao and Xiaolin Chen from Henan University of Technology suggests shifting towards advanced techniques for more accurate news summaries.\\n\\nThe development of a News Summary Generator (NSG) is discussed, combining genetic algorithms and LLM to extract and refine event patterns for accurate summaries. The importance of pre-trained language models, particularly Transformer-based models, in enhancing news summary efficiency and accuracy is emphasized. The NSG successfully generates informative summaries with a degree of generalization.\\n\\nTraining on large-scale textual data has led to pre-trained LLMs becoming widely applicable in various domains, including news summary generation. LLMs have shown improvements in natural language processing tasks, making them valuable for producing concise event summaries. Accurate and comprehensive information aids in understanding real-world events, making news summary generation crucial for providing valuable insights to users and facilitating informed decision-making.\\n\\nIn conclusion, the document underscores the importance of accurate and comprehensive information for individuals to understand complex events. By utilizing advanced algorithms and technologies like LLMs and genetic algorithms, news summary generation can offer accurate and insightful summaries, empowering users to navigate current events effectively.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
